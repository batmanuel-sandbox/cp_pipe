#
# LSST Data Management System
#
# Copyright 2008-2017  AURA/LSST.
#
# This product includes software developed by the
# LSST Project (http://www.lsst.org/).
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the LSST License Statement and
# the GNU General Public License along with this program.  If not,
# see <https://www.lsstcorp.org/LegalNotices/>.
#

"""Calculation of brighter-fatter effect correlations and kernels."""
from __future__ import print_function

from builtins import zip
from builtins import str
from builtins import range
import os
from scipy import stats
from mpl_toolkits.mplot3d import axes3d  # not actually unused, required for 3d projection
import numpy as np
import matplotlib.pyplot as plt

import lsstDebug
import lsst.afw.image as afwImage
import lsst.afw.math as afwMath
import lsst.afw.display as afwDisp
import lsst.pex.config as pexConfig
import lsst.pipe.base as pipeBase
from lsst.obs.subaru.crosstalk import CrosstalkTask
from lsst.obs.subaru.isr import SubaruIsrTask


class BfTaskConfig(pexConfig.Config):
    """Config class for bright-fatter effect coefficient calculation."""

    doCalcGains = pexConfig.Field(
        dtype=bool,
        doc="Measure the per-amplifier gains using the PTC method",
        default=True,
    )
    maxIterRegression = pexConfig.Field(
        dtype=int,
        doc="Maximum number of iterations for the iterative regression fitter",
        default=10
    )
    nSigmaClipGainCalc = pexConfig.Field(
        dtype=int,
        doc="Number of sigma to clip to during gain calculation",
        default=5
    )
    nSigmaClipRegression = pexConfig.Field(
        dtype=int,
        doc="Number of sigma to clip to during iterative regression",
        default=3
    )
    xcorrCheckRejectLevel = pexConfig.Field(
        dtype=float,
        doc="Sanity check level for the sum of the input cross-correlations. Arrays which "
        "sum to greater than this are discarded before the clipped mean is calculated.",
        # default=1000.0  # xxx change this back once the problem is fixed!
        default=0.2
    )
    maxIterSOR = pexConfig.Field(
        dtype=int,
        doc="The maximum number of iterations allowed for the successive over-relaxation method",
        default=10000
    )
    eLevelSOR = pexConfig.Field(
        dtype=float,
        doc="The target residual error for the successive over-relaxation method",
        default=5.0e-14
    )
    nSigmaClipKernelGen = pexConfig.Field(
        dtype=float,
        doc="Number of sigma to clip to during pixel-wise clipping when generating the kernel",
        default=4
    )
    nSigmaClipXCorr = pexConfig.Field(
        dtype=float,
        doc="Number of sigma to clip when calculating means for the cross correlation",
        default=5
    )
    maxLag = pexConfig.Field(
        dtype=int,
        doc="The maximum lag to use when calculating the cross correlation/kernel",
        default=5
    )
    nPixBorderGainCalc = pexConfig.Field(
        dtype=int,
        doc="The number of border pixels to exclude when calculating the gain",
        default=10
    )
    nPixBorderXCorr = pexConfig.Field(
        dtype=int,
        doc="The number of border pixels to exclude when calculating the cross correlation/kernel",
        default=10
    )
    biasCorr = pexConfig.Field(
        dtype=float,
        doc="A scary, empirically determined correction-factor correcting for the sigma-clipping" +
        " a non-Gaussian distribution",
        default=0.9241
    )
    backgroundBinSize = pexConfig.Field(
        dtype=int,
        doc="Size of the background bins",
        default=128
    )
    fixPtcThroughOrigin = pexConfig.Field(
        dtype=bool,
        doc="Contrain the fit of the PTC to go through the origin?",
        default=True
    )
    level = pexConfig.ChoiceField(
        doc="The level at which to calculate kernels",
        dtype=str, default="CCD",
        allowed={
            "AMP": "Every amplifier treated separately",
            "CCD": "One kernel per CCD",
            # "VENDOR": "Should this be implemented? How would that work?",
            # "FOCAL_PLANE": "This seems unwise and probably shouldn't exist",
        }
    )


class BfTaskRunner(pipeBase.TaskRunner):
    """Subclass of TaskRunner for the bfTask.

    This transforms the processed arguments generated by the ArgumentParser into the arguments expected by
    bfTask.run().

    bfTask.run() takes a two arguments, one of which is the dataRef (as usual), and the other is the
    list of visitPairs, in the form of a list of tuples. This list is supplied on the command line as
    documented, and this class parses that, and passes the parsed version to the run() method.

    See pipeBase.TaskRunner for more information.
    """

    @staticmethod
    def getTargetList(parsedCmd, **kwargs):
        """Parse the visit list and pass through explicitly."""
        visitPairs = parsedCmd.visitPairs

        tuples = visitPairs.split("),(")  # split
        tuples[0] = tuples[0][1:]  # remove leading "(
        tuples[-1] = tuples[-1][:-1]  # remove trailing )"
        visitPairs = [(int(v1), int(v2)) for (v1, v2) in [tup.split(',') for tup in tuples]]  # cast to int

        # if the reviewer is a die-hard regex fan then uncomment the below, but the above is more readable
        # visitPairs = re.findall(r'\((\d+),\s*(\d+)\)', visitPairs)  # break down visit pair list
        # visitPairs = [tuple([int(y) for y in x]) for x in visitPairs]  # make a tuple of ints
        return pipeBase.TaskRunner.getTargetList(parsedCmd, visitPairs=visitPairs, **kwargs)


class BfDataIdContainer(pipeBase.DataIdContainer):
    """A DataIdContainer for the BF task."""

    def makeDataRefList(self, namespace):
        """Compute refList based on idList.

        This method must be defined as the dataset does not exist before this task is run.

        Parameters
        ----------
        namespace
            Results of parsing command-line (with ``butler`` and ``log`` elements).

        Notes
        -----
        Not called if ``add_id_argument`` called with ``doMakeDataRefList=False``.
        Note that this is almost a copy-and-paste of the vanilla implementation, but without checking
        if the datasets already exist as this task exists to make them.
        """
        if self.datasetType is None:
            raise RuntimeError("Must call setDatasetType first")
        butler = namespace.butler
        for dataId in self.idList:
            refList = list(butler.subset(datasetType=self.datasetType, level=self.level, dataId=dataId))
            # exclude nonexistent data
            # this is a recursive test, e.g. for the sake of "raw" data
            if not refList:
                namespace.log.warn("No data found for dataId=%s", dataId)
                continue
            self.refList += refList


class BfTask(pipeBase.CmdLineTask):
    """Bright-fatter effect coefficient calculation task.

    See http://ls.st/ldm-151 Chapter 4, Calibration Products Production for further details
    regarding the inputs and outputs.
    """

    RunnerClass = BfTaskRunner
    ConfigClass = BfTaskConfig
    _DefaultName = "bf"

    def __init__(self, *args, **kwargs):
        """Constructor for the BfTask."""
        pipeBase.CmdLineTask.__init__(self, *args, **kwargs)

        self.debug = lsstDebug.Info(__name__)
        if self.debug.enabled:
            self.log.info("Running with debug enabled...")
            # if we're displaying, test it works, and save the displays for later
            # it's worth testing here as displays are flaky and sometimes can't be contacted
            # and given processing takes a while, it's a shame to fail late due to display issues
            if self.debug.display:
                try:
                    afwDisp.setDefaultBackend(self.debug.displayBackend)
                    afwDisp.Display.delAllDisplays()
                    self.disp1 = afwDisp.Display(0, open=True)
                    self.disp2 = afwDisp.Display(1, open=True)

                    im = afwImage.ImageF(1, 1)
                    im.array[:] = [[1]]
                    self.disp1.mtv(im)
                    self.disp1.erase()
                except NameError:
                    self.debug.display = False
                    self.log.warn('Failed to setup/connect to display! Debug display has been disabled')

        self.config.validate()
        self.config.freeze()

    @classmethod
    def _makeArgumentParser(cls):
        """Augment argument parser for the BfTask."""
        parser = pipeBase.ArgumentParser(name=cls._DefaultName)
        parser.add_argument("--visitPairs", help="The list of visit pairs to use, as a list of tuples "
                            "enclosed in quotes e.g. \"(123,456),(789,987),(654,321)\""
                            "NB: must be comma-separated-tuples with no spaces, enclosed in quotes!")
        parser.add_id_argument("--id", datasetType="bfKernelNew", ContainerClass=BfDataIdContainer,
                               help="The ccds to use, e.g. --id ccd=0..100")
        return parser

    @pipeBase.timeMethod
    def run(self, dataRef, visitPairs):
        """Run the brighter-fatter measurement task.

        For a dataRef (which is each ccd here), and given a list of visit pairs, calulate the
        brighter-fatter kernel for the ccd.

        TODO: might want to have some syntax for ganging ccds together
        #    though this might just be all-together, or all separate actually
        #    it's possible we could gang together on boule/batch, but let's not worry for now

        Parameters
        ----------
        dataRef : list of lsst.daf.persistence.ButlerDataRef
            dataRef for the CCD for the visits to be fit.
        visitPairs : `iterable` of `tuple` of `int`
            Pairs of visit numbers to be processed together
        """
        # self.xxx_test_estimateGains()
        # self.xxx_test_generateKernel()
        # self.xxx_test_xcorr()
        # self.xxx_test_put(dataRef)

        gains = []
        xcorrs = {}  # dict of lists keyed by either amp or CCD depending on level
        means = {}
        kernels = {}

        ccdNum = dataRef.dataId['ccd']

        # calculate or retrieve the gains
        if self.config.doCalcGains:
            self.log.info('Beginning gain estimation for CCD %s'%ccdNum)
            gains, nomGains = self.estimateGains(dataRef, visitPairs)
            dataRef.put(gains, datasetType='bfGain')
            self.log.info('Finished gain estimation for CCD %s'%ccdNum)
        else:
            gains = dataRef.get('bfGain')
            if not gains:
                self.log.fatal('Failed to retrieved gains for CCD %s'%ccdNum)
                raise RuntimeError("Must either calculate or supply gains for %s"%ccdNum)
            self.log.info('Retrieved stored gain for CCD %s'%ccdNum)
        self.log.debug('CCD %s has gains %s'%(ccdNum, gains))

        # Loop over pairs of visits, calculating the cross correlations for the required level
        # xcorrFromVisitPair returns dicts of the xcorrs and means, keyed depending on level
        for (v1, v2) in visitPairs:
            self.log.info('Beginning cross corellation calculation for CCD %s'%ccdNum)
            # xcorr, mean = self.xcorrFromVisitPair(dataRef, v1, v2, gains=gains, level=self.level)
            retXcorr, retMean = self.xcorrFromVisitPair(dataRef, v1, v2, gains, self.level)
            for k in retXcorr.keys():  # unpack the results into the corresponding dicts
                if not xcorrs[k]:
                    xcorrs[k] = []
                    means[k] = []
                xcorrs[k].append(retXcorr[k])
                means[k].append(retMean[k])

        # generate the kernel
        for key in xcorrs.keys():
            kernels[key] = self._generateKernel(xcorrs[key], means[key])
            dataRef.put(kernels)

        # # generate the kernel
        # if self.level == 'CCD':
        #     self.log.info('Generating kernel for CCD %s'%ccdNum)
        #     kernel = self._generateKernel(xcorrs, means)
        #     dataRef.put(kernel)
        # else:  # currently this is only 'AMP'
        #     self.log.info('Generating kernels for CCD %s'%ccdNum)
        #     kernels = self._generateKernel(xcorrs, means, level=self.level)
        #     dataRef.put(kernels)

        self.log.info('Finished generating kernel(s) for %s'%ccdNum)
        return pipeBase.Struct(exitStatus=0)

    def xcorrFromVisitPair(self, dataRef, v1, v2, gains, level):
        """Return the cross-correlation from a given pair of visits.

        This is code preforms some preliminary operations and then calls the main correlation calc code.
        This is used for calculating the xcorr after setting the gains.

        Parameters:
        -----------
        dataRef : list of lsst.daf.persistence.ButlerDataRef
            dataRef for the CCD for the visits to be fit.
        v1 : `int`
            Visit number of the first visit
        v2 : `int`
            Visit number of the second visit
        gains : `dict`
            Dictionary of gains for each amplifier in the CCD. Keys are amp name, values are gains
        level : `str`
            The level at which to calculate the kernel, either 'AMP' or 'CCD'

        Returns:
        xcorr : `dict` of `np.array`
            The cross-correlation numpy arrays in a dict, keyed depending on the level
        means : `list` of `float`
            The sigma-clipped-mean flux in the input images in a dict, keyed depending on the level
        """
        im1 = self.isr(dataRef, v1)
        im2 = self.isr(dataRef, v2)

        xcorrDict = {}
        meanDict = {}

        detector = dataRef.get('raw_detector')

        if level == 'CCD':
            xcorr, xcorrMeans = self._xcorr(im1, im2, gains, level)
            detId = detector.getId()
            xcorrDict[detId] = xcorr  # note that we key by ID *not* name here (unlike for amps)
            meanDict[detId] = xcorrMeans  # this is following the way in which we loop over detectors
        else:
            ampInfoCat = detector.getAmpInfoCatalog()
            for ampNum, ampInfo in enumerate(ampInfoCat):
                bbox = ampInfo.getBBox()
                ampName = ampInfo.getName()
                gain = gains[ampName]
                area1 = im1[bbox]
                area2 = im2[bbox]
                xcorr, xcorrMeans = self._newxcorr(area1, area2, gain)
                xcorrDict[ampName] = xcorr
                meanDict[ampName] = xcorrMeans

        return xcorrDict, meanDict

        # xcorr, xcorrMeans = self._xcorr(im1, im2, gains)

        # if self.debug.enabled:
        #     means = [afwMath.makeStatistics(im.getMaskedImage(),
        #                                     afwMath.MEANCLIP).getValue() for im in [im1, im2]]
        #     ccdNum = dataRef.dataId['ccd']
        #     title = "Visits %s; %s, CCDs %s <I> = %.3f (%s) Var = %.4f"%(self._getNameOfSet([v1]),
        #                                                                  self._getNameOfSet([v2]),
        #                                                                  self._getNameOfSet([ccdNum]),
        #                                                                  xcorrMeans[0]+means[1],
        #                                                                  im1.getFilter().getName(),
        #                                                                  float(xcorr[0, 0]) /
        #                                                                       (xcorrMeans[0]+means[1]))
        #     fileName = (os.path.join(self.debug.debugPlotPath, '_'.join(['xcorr_visit', str(v1),
        #                                                                  str(v2), 'ccd', str(ccdNum)])))
        #     fileName += self.debug.plotType
        #     self._plotXcorr(xcorr.copy(), (xcorrMeans[0]+means[1]),
        #                     title=title, save=True, fileName=fileName)

        # return xcorr, xcorrMeans

    def _newxcorr(self, im1, im2, gains, level):
        """Calculate the cross-correlation of two images im1 and im2 using robust measures of the covariance.

        Parameters:
        -----------
        im1 : `afwImage.Exposure` or similar
            The first image-like object, to be cross-correlated with the second
        im2 : `afwImage.Exposure` or similar
            The second image-like object, to be cross-correlated with the first
        gains : `list` of `float`
            The per-amplifier gains for the detector

        Returns:
        --------
        xcorr : `np.array`
            The quarter-image cross-corellation
        means : `list` of `float`
            The as-calculated means of the input images (clipped, and with borders applied)

        Notes:
        ------
        This function is controlled by the following pexConfig parameters:

        maxLag : `int`
            The maximum lag to use in the cross-correlation calculation
        nPixBorderXCorr : `int`
            The number of border pixels to exclude
        nSigmaClipXCorr : `float`
            The number of sigma to be clipped to
        biasCorr : `float`
            Parameter used to correct from the bias introduced by the sigma cuts.
        """
        maxLag = self.config.maxLag
        border = self.config.nPixBorderXCorr
        sigma = self.config.nSigmaClipXCorr
        biasCorr = self.config.biasCorr

        try:  # only for debug, but used in more than one place
            frameId1 = im1.getMetadata().get("FRAMEID")
            frameId2 = im2.getMetadata().get("FRAMEID")
            frameId = '_diff_'.join(frameId1, frameId2)
        except:
            frameId = 'Im1 diff Im2'

        sctrl = afwMath.StatisticsControl()
        sctrl.setNumSigmaClip(sigma)

        im1 = self._convertImagelikeToFloatImage(im1)
        im2 = self._convertImagelikeToFloatImage(im2)

        means = {}
        xcorrs = {}
        for imNum, im in enumerate([im1, im2]):
            detector = im.getDetector()
            ampInfoCat = detector.getAmpInfoCatalog()
            temp = im.clone()
            # Rescale each amp by the appropriate gain and subtract the mean.
            # NB these are views modifying the image in-place
            for amp in ampInfoCat:
                ampName = amp.getName()
                smi = im[amp.getBBox()]  # the soon-to-be scaled, mean subtractedm, amp image
                smiTemp = temp[amp.getBBox()]
                mean = afwMath.makeStatistics(smi, afwMath.MEANCLIP, sctrl).getValue()
                gain = gains[ampName]
                smi *= gain
                smiTemp *= gain
                self.log.debug(mean*gain, afwMath.makeStatistics(smi, afwMath.MEANCLIP, sctrl).getValue())
                smi -= mean*gain
                if level == 'AMP':  # build the dicts if doing amp-wise
                    ampMean = afwMath.makeStatistics(smiTemp[border:-border, border:-border],
                                                     afwMath.MEANCLIP, sctrl).getValue()
                    if not means[ampName]:
                        means[ampName] = []
                    means[ampName].append(ampMean)
            if level == 'CCD':  #  else just average the whole CCD
                # xxx I think temp can be done away with by just adding the mean here, right?
                means[imNum] = afwMath.makeStatistics(temp[border:-border, border:-border],
                                                      afwMath.MEANCLIP, sctrl).getValue()

        # TODO: Make this part a per-amplifier dict depending on level!
        # Actually diff the images
        diff = im1.clone()
        diff = diff.getMaskedImage().getImage()  # xxx haven't we already guaranteed that this is an image?
        diff -= im2.getMaskedImage().getImage()
        diff = diff[border:-border, border:-border]

        if self.debug.writeDiffImages:
            filename = '_'.join(['diff', 'CCD', str(detector.getId()), frameId, '.fits'])
            diff.writeFits(os.path.join(self.debug.debugDataPath, filename))

        # Subtract background.  It should be a constant, but it isn't always
        # xxx do we want some logic here for whether to subtract or not?
        binsize = self.config.backgroundBinSize
        nx = diff.getWidth()//binsize
        ny = diff.getHeight()//binsize
        bctrl = afwMath.BackgroundControl(nx, ny, sctrl, afwMath.MEANCLIP)
        bkgd = afwMath.makeBackground(diff, bctrl)
        diff -= bkgd.getImageF(afwMath.Interpolate.CUBIC_SPLINE, afwMath.REDUCE_INTERP_ORDER)

        if self.debug.writeDiffImages:
            filename = '_'.join(['bgSub', 'diff', 'CCD', str(detector.getId()), frameId, '.fits'])
            diff.writeFits(os.path.join(self.debug.debugDataPath, filename))
        if self.debug.display:
            self.disp1.mtv(diff, title=frameId)

        self.log.debug("Median and variance of diff:")
        self.log.debug(afwMath.makeStatistics(diff, afwMath.MEDIAN, sctrl).getValue())
        self.log.debug(afwMath.makeStatistics(diff, afwMath.VARIANCECLIP,
                                              sctrl).getValue(), np.var(diff.getArray()))

        # Measure the correlations
        dim0 = diff[0: -maxLag, : -maxLag]
        dim0 -= afwMath.makeStatistics(dim0, afwMath.MEANCLIP, sctrl).getValue()
        w, h = dim0.getDimensions()
        # default dict, or dict.fromKeys() here maybe? Perhaps same for above?
        xcorr = np.zeros((maxLag + 1, maxLag + 1), dtype=np.float64)

        for xlag in range(maxLag + 1):
            for ylag in range(maxLag + 1):
                dim_xy = diff[xlag:xlag + w, ylag: ylag + h].clone()
                dim_xy -= afwMath.makeStatistics(dim_xy, afwMath.MEANCLIP, sctrl).getValue()
                dim_xy *= dim0
                xcorr[xlag, ylag] = afwMath.makeStatistics(dim_xy,
                                                           afwMath.MEANCLIP, sctrl).getValue()/(biasCorr)

        xcorr_full = self._tileArray(xcorr)
        self.log.debug(sum(means), xcorr[0, 0], np.sum(xcorr_full), xcorr[0, 0]/sum(means),
                       np.sum(xcorr_full)/sum(means))
        return (xcorr, means)

    def xcorrFromVisitPair_old(self, dataRef, v1, v2, gains, level):
        """Return the cross-correlation from a given pair of visits.

        This is code preforms some preliminary operations and then calls the main correlation calc code.
        This is used for calculating the xcorr after setting the gains.

        Parameters:
        -----------
        dataRef : list of lsst.daf.persistence.ButlerDataRef
            dataRef for the CCD for the visits to be fit.
        v1 : `int`
            Visit number of the first visit
        v2 : `int`
            Visit number of the second visit
        gains : `iterable` of `float`
            The gains for each amplifier in the CCD
        level : `str`
            The level at which to calculate the kernel, either 'AMP' or 'CCD'

        Returns:
        xcorr : `dict` of `np.array`
            The cross-correlation numpy arrays in a dict, keyed depending on the level
        means : `list` of `float`
            The sigma-clipped-mean flux in the input images in a dict, keyed depending on the level
        """
        im1 = self.isr(dataRef, v1)
        im2 = self.isr(dataRef, v2)
        xcorr, xcorrMeans = self._xcorr(im1, im2, gains)

        if self.debug.enabled:
            means = [afwMath.makeStatistics(im.getMaskedImage(),
                                            afwMath.MEANCLIP).getValue() for im in [im1, im2]]
            ccdNum = dataRef.dataId['ccd']
            title = "Visits %s; %s, CCDs %s <I> = %.3f (%s) Var = %.4f"%(self._getNameOfSet([v1]),
                                                                         self._getNameOfSet([v2]),
                                                                         self._getNameOfSet([ccdNum]),
                                                                         xcorrMeans[0]+means[1],
                                                                         im1.getFilter().getName(),
                                                                         float(xcorr[0, 0]) /
                                                                              (xcorrMeans[0]+means[1]))
            fileName = (os.path.join(self.debug.debugPlotPath, '_'.join(['xcorr_visit', str(v1),
                                                                         str(v2), 'ccd', str(ccdNum)])))
            fileName += self.debug.plotType
            self._plotXcorr(xcorr.copy(), (xcorrMeans[0]+means[1]),
                            title=title, save=True, fileName=fileName)

        return xcorr, xcorrMeans

    def xxx_test_put(self, dataRef):
        """Xxx Docstring."""
        from time import sleep
        print('Starting, if parallelised this will be printed -j times simultaneously')
        sleep(4)
        print('finished')
        a = np.zeros((3, 3))
        dataRef.put(a)
        return

    def xxx_test_estimateGains(self):
        """Xxx Docstring."""
        import lsst.daf.persistence as dafPersist
        butler = dafPersist.Butler('/datasets/hsc/repo/')
        visPairs = [(904606, 904608),
                    (904610, 904612)]
        ignoreCcdList = [_ for _ in range(112)]
        ignoreCcdList.remove(41)
        self.estimateGains(butler, visPairs, ignoreCcdList)

    def xxx_test_generateKernel(self):
        """Docstring."""
        import pickle
        f = open('/home/mfl/bf_output/merlinTestXcorr.pkl', 'rb')
        xcorr, means = pickle.load(f)
        f.close()
        self.log.info('\n\nLevel = %s\n\n'%self.config.xcorrCheckRejectLevel)
        kernel = self._generateKernel(xcorr, means)
        f = open('/home/mfl/bf_output/taskOutput_kernel.pkl', 'wb')
        pickle.dump(kernel, f)
        f.close()

    def estimateGains(self, dataRef, visitPairs):
        """Estimate the gains of the amplifiers in the CCD using the specified visits.

        Given a dataRef and list of flats of varying intensity, calculate the gain for each
        CCD specified using the PTC method.

        The fixPtcThroughOrigin config option determines whether the iterative fitting is
        forced to go through the origin or not. This defaults to True, fitting var=1/gain * mean,
        if set to False then var=1/g * mean + const is fitted.

        TODO: DM-14063
        This is really a PTC gain measurement method. So we should compare results from this
        task to the eotest PTC gain measurements once that's ported

        Parameters
        ----------
        dataRef : `lsst.daf.persistence.butler.Butler.dataRef`
            dataRef for the CCD for the flats to be used
        visitPairs : `list` of `tuple`
            List of visit-pairs to use, as [(v1,v2), (v3,v4)...]

        Returns
        -------
        gains : `dict` of `float`
            List of the as-calculated amplifier gain values
        nominalGains : `dict` of `float`
            The amplifier gains, as reported by the `detector` object
        """
        ampMeans = []
        ampVariances = []
        ampCorrVariances = []
        ampGains = []
        nomGains = []

        # Loop over the amps in the CCD, calculating a PTC for each amplifier.
        # The amplifier iteration is performed in _calcMeansAndVars()
        # NB: no gain correction is applied
        for visPairNum, visPair in enumerate(visitPairs):
            _means, _vars, _covars, _gains = self._calcMeansAndVars(dataRef, visPair[0], visPair[1])
            breaker = 0
            # Do sanity checks; if these are failed more investigation is needed!
            for i, j in enumerate(_means):
                if _means[i]*10 < _vars[i] or _means[i]*10 < _covars[i]:
                    self.log.warn('Sanity check failed; check visit %s'%visPair)
                    breaker += 1
            if breaker:
                continue
            if visPairNum == 0:
                for i in range(len(_means)):
                    ampMeans.append(np.array([]))
                    ampVariances.append(np.array([]))
                    ampCorrVariances.append(np.array([]))
                    ampGains.append(np.array([]))
            for i, j in enumerate(_means):
                if visPairNum == 0:
                    nomGains.append(_gains[i])
                if _vars[i]*1.3 < _covars[i] or _vars[i]*0.7 > _covars[i]:
                    continue
                ampMeans[i] = np.append(ampMeans[i], _means[i])
                ampVariances[i] = np.append(ampVariances[i], _vars[i])
                ampCorrVariances[i] = np.append(ampCorrVariances[i], _covars[i])
                ampGains[i] = np.append(ampGains[i], _gains[i])

        gains = []
        for i in range(len(ampMeans)):
            slopeRaw, interceptRaw, rVal, pVal, stdErr = stats.linregress(ampMeans[i], ampCorrVariances[i])
            slopeFix, _ = self._iterativeRegression(ampMeans[i], ampCorrVariances[i], fixThroughOrigin=True)
            slopeUnfix, intercept = self._iterativeRegression(ampMeans[i], ampCorrVariances[i],
                                                              fixThroughOrigin=False)
            self.log.info("Slope of     raw fit: %s  intercept: %s p value: %s"%(slopeRaw,
                                                                                 interceptRaw, pVal))
            self.log.info("slope of   fixed fit: %s, difference vs raw:%s"%(slopeFix,
                                                                            slopeFix-slopeRaw))
            self.log.info("slope of unfixed fit: %s, difference vs fix:%s"%(slopeUnfix,
                                                                            slopeFix-slopeUnfix))
            if self.config.fixPtcThroughOrigin:
                slopeToUse = slopeFix
            else:
                slopeToUse = slopeUnfix

            if self.debug.enabled:
                fig = plt.figure()
                ax = fig.add_subplot(111)
                ax.plot(ampMeans[i], ampCorrVariances[i], linestyle='None', marker='x', label='data')
                if self.config.fixPtcThroughOrigin:
                    ax.plot(ampMeans[i], ampMeans[i]*slopeToUse, label='Fit through origin')
                else:
                    ax.plot(ampMeans[i], ampMeans[i]*slopeToUse+intercept,
                            label='Fit (intercept unconstrained')
                ccdNum = dataRef.dataId['ccd']
                title = '_'.join(['PTC_CCD', str(ccdNum), 'AMP', str(i), self.debug.plotType])
                fileName = os.path.join(self.debug.debugPlotPath, title)
                fig.savefig(fileName)
                self.log.info('Saved PTC to %s'%fileName)
            gains.append(1.0/slopeToUse)
        return gains, nomGains

    def _calcMeansAndVars(self, dataRef, v1, v2):
        """Calculate the means, vars, covars, and retieve the nominal gains, for each amp in each ccd.

        This code runs using two visit numbers, and for ccd specified.
        It calculates the correlations in the individual amps without rescaling any gains.
        This allows a photon transfer curve to be generated and the gains measured.

        Images are assembled with use the isrTask, and basic isr is performed.
        Note that the isr task used MUST set the EDGE bits.[xxx need to change to using this, or change this]

        Parameters:
        -----------
        dataRef : `lsst.daf.persistence.butler.Butler.dataRef`
            dataRef for the CCD for the repo containg the flats to be used
        v1 : `int`
            First visit of the visit pair
        v2 : `int`
            Second visit of the visit pair

        Returns
        -------
        means, vars, covars, gains : `tuple` of `lists`
            The sum of the means, variance, one quarter of the xcorr, and the original gain for each amp.
        """
        sigma = self.config.nSigmaClipGainCalc
        maxLag = self.config.maxLag
        border = self.config.nPixBorderGainCalc
        biasCorr = self.config.biasCorr

        nomGains = []
        imMeans = [None, None]
        ampMeans = [[], []]

        ims = [self.isr(dataRef, v1), self.isr(dataRef, v2)]
        if self.debug.display:
            self.disp1.mtv(ims[0], title=str(v1))
            self.disp2.mtv(ims[1], title=str(v2))

        sctrl = afwMath.StatisticsControl()
        sctrl.setNumSigmaClip(sigma)
        for imNum, im in enumerate(ims):
            ccd = im.getDetector()
            # Starting with an Exposure, MaskedImage, or Image trim the data and convert to float
            im = self._convertImagelikeToFloatImage(im)

            # calculate the sigma-clipped mean, excluding the borders
            # TODO: rewrite to use egde bits
            imMeans[imNum] = afwMath.makeStatistics(im, afwMath.MEANCLIP, sctrl).getValue()
            for ampNum, amp in enumerate(ccd):
                ampIm = im[amp.getBBox()]
                if ampNum == 0:
                    mean = afwMath.makeStatistics(ampIm[border:, border:-border],
                                                  afwMath.MEANCLIP, sctrl).getValue()
                elif ampNum == 3:
                    mean = afwMath.makeStatistics(ampIm[:-border, border:-border],
                                                  afwMath.MEANCLIP, sctrl).getValue()
                else:
                    mean = afwMath.makeStatistics(ampIm[:, border:-border],
                                                  afwMath.MEANCLIP, sctrl).getValue()
                nomGain = amp.getGain()
                ampMeans[imNum].append(mean)
                if imNum == 0:
                    nomGains.append(nomGain)
                ampIm -= mean

        diff = ims[0].clone()
        diff = diff.getMaskedImage().getImage()
        diff -= ims[1].getMaskedImage().getImage()

        temp = diff[border:-border, border:-border]

        # Subtract background.  It should be a constant, but it isn't always (e.g. some SuprimeCam flats)
        # TODO: Check how this looks, and if this is the "right" way to do this
        binsize = self.config.backgroundBinSize
        nx = temp.getWidth()//binsize
        ny = temp.getHeight()//binsize
        bctrl = afwMath.BackgroundControl(nx, ny, sctrl, afwMath.MEANCLIP)
        bkgd = afwMath.makeBackground(temp, bctrl)
        diff[border:-border, border:-border] -= bkgd.getImageF(afwMath.Interpolate.CUBIC_SPLINE,
                                                               afwMath.REDUCE_INTERP_ORDER)
        variances = []  # can't shadow builtin "vars"
        coVars = []
        # For each amp calculate the correlation
        # xxx can you do this for a heterogenous focal plane? (answer: 100% no)
        # xxx update note to self - now that we're doing ccd by ccd with a dataRef this is fine again
        CCD = ims[0].getDetector()
        for ampNum, amp in enumerate(CCD):
            borderL = 0
            borderR = 0
            if ampNum == 0:  # TODO: this needs rewriting for using edge bits to make camera agnostic
                borderL = border
            if ampNum == 3:
                borderR = border

            diffAmpIm = diff[amp.getBBox()].clone()  # xxx why is this a clone? move .clone() to next line?
            diffAmpImCrop = diffAmpIm[borderL:-borderR-maxLag, border:-border-maxLag]
            diffAmpImCrop -= afwMath.makeStatistics(diffAmpImCrop, afwMath.MEANCLIP, sctrl).getValue()
            w, h = diffAmpImCrop.getDimensions()
            xcorr = np.zeros((maxLag + 1, maxLag + 1), dtype=np.float64)

            # calculate the cross correlation
            for xlag in range(maxLag + 1):
                for ylag in range(maxLag + 1):
                    dim_xy = diffAmpIm[borderL+xlag:borderL+xlag + w, border+ylag: border+ylag + h].clone()
                    dim_xy -= afwMath.makeStatistics(dim_xy, afwMath.MEANCLIP, sctrl).getValue()
                    dim_xy *= diffAmpImCrop
                    xcorr[xlag, ylag] = afwMath.makeStatistics(dim_xy,
                                                               afwMath.MEANCLIP, sctrl).getValue()/(biasCorr)

            variances.append(xcorr[0, 0])
            xcorr_full = self._tileArray(xcorr)
            coVars.append(np.sum(xcorr_full))

            msg = "M1: " + str(ampMeans[0][ampNum])
            msg += " M2 " + str(ampMeans[1][ampNum])
            msg += " M_sum: " + str((ampMeans[0][ampNum])+ampMeans[1][ampNum])
            msg += " Var " + str(variances[ampNum])
            msg += " coVar: " + str(coVars[ampNum])
            self.log.debug(msg)
        return ([i+j for i, j in zip(ampMeans[1], ampMeans[0])], variances, coVars, nomGains)

    def isr(self, dataRef, visit):  # TODO: Need to replace this with a retargetable ISR task
        """Some simple code to perform some simple ISR."""
        dataRef.dataId['visit'] = visit
        config = SubaruIsrTask.ConfigClass()
        # config.load(os.path.join(os.environ["OBS_SUBARU_DIR"], "config", "isr.py"))
        # config.load(os.path.join(os.environ["OBS_SUBARU_DIR"], "config", "hsc", "isr.py"))

        config.doFlat = False
        config.doGuider = False
        config.doSaturation = True
        config.doWrite = False
        config.doDefect = True
        config.qa.doThumbnailOss = False
        config.qa.doThumbnailFlattened = False
        config.doFringe = False
        config.fringe.filters = ['y', ]
        config.overscanFitType = "AKIMA_SPLINE"
        config.overscanOrder = 30
        config.doAttachTransmissionCurve = False
        # Overscan is fairly efficient at removing bias level, but leaves a line in the middle
        config.doBias = True
        config.doDark = True  # Required especially around CCD 33
        config.crosstalk.retarget(CrosstalkTask)
        config.crosstalk.value.coeffs.values = [0.0e-6, -125.0e-6, -149.0e-6, -156.0e-6, -124.0e-6, 0.0e-6, -
                                                132.0e-6, -157.0e-6, -171.0e-6, -134.0e-6, 0.0e-6, -153.0e-6,
                                                -157.0e-6, -151.0e-6, -137.0e-6, 0.0e-6, ]
        isr = SubaruIsrTask(config=config)
        exp = isr.run(dataRef).exposure
        return exp

    def _plotXcorr(self, xcorr, mean, zmax=0.05, title=None, fig=None, save=False, fileName=None):
        """Used to plot the correlation functions."""
        try:
            xcorr = xcorr.getArray()
        except:
            pass

        xcorr /= float(mean)
        # xcorr.getArray()[0,0]=abs(xcorr.getArray()[0,0]-1)

        if fig is None:
            fig = plt.figure()
        else:
            fig.clf()

        ax = fig.add_subplot(111, projection='3d')
        ax.azim = 30
        ax.elev = 20

        nx, ny = np.shape(xcorr)

        xpos, ypos = np.meshgrid(np.arange(nx), np.arange(ny))
        xpos = xpos.flatten()
        ypos = ypos.flatten()
        zpos = np.zeros(nx*ny)
        dz = xcorr.flatten()
        dz[dz > zmax] = zmax

        ax.bar3d(xpos, ypos, zpos, 1, 1, dz, color='b', zsort='max', sort_zpos=100)
        if xcorr[0, 0] > zmax:
            ax.bar3d([0], [0], [zmax], 1, 1, 1e-4, color='c')

        ax.set_xlabel("row")
        ax.set_ylabel("column")
        ax.set_zlabel(r"$\langle{(F_i - \bar{F})(F_i - \bar{F})}\rangle/\bar{F}$")

        if title:
            fig.suptitle(title)
        if save is True:
            fig.savefig(fileName)

    @staticmethod
    def _getNameOfSet(vals):
        """Convert a list of numbers into a string, merging consecutive values."""
        if not vals:
            return ""

        def _addPairToName(valName, val0, val1):
            """Add a pair of values, val0 and val1, to the valName list."""
            sval1 = str(val1)
            if val0 != val1:
                pre = os.path.commonprefix([str(val0), sval1])
                sval1 = int(sval1[len(pre):])
            valName.append("%s-%s" % (val0, sval1) if val1 != val0 else str(val0))

        valName = []
        val0 = vals[0]
        val1 = val0
        for val in vals[1:]:
            if isinstance(val, int) and val == val1 + 1:
                val1 = val
            else:
                _addPairToName(valName, val0, val1)
                val0 = val
                val1 = val0

        _addPairToName(valName, val0, val1)

        return ", ".join(valName)

    def _iterativeRegression(self, x, y, fixThroughOrigin=False, nSigmaClip=None, maxIter=None):
        """Use linear regression to fit a line of best fit, iteratively removing outliers.

        Useful when you have sufficiently large numbers of points on your PTC.
        Function iterates until either there are no outliers of nSigmaClip magnitude, or until the specified
        max number of iterations have been performed.

        Parameters:
        -----------
        x : `numpy.array`
            The independent variable
        y : `numpy.array`
            The dependent variable

        Returns:
        --------
        slope : `float`
            The slope of the line of best fit
        intercept : `float`
            The y-intercept of the line of best fit
        """
        if not maxIter:
            maxIter = self.config.maxIterRegression
        if not nSigmaClip:
            nSigmaClip = self.config.nSigmaClipRegression

        nIter = 0
        sctrl = afwMath.StatisticsControl()
        sctrl.setNumSigmaClip(nSigmaClip)

        if fixThroughOrigin:
            while nIter < maxIter:
                nIter += 1
                self.log.debug("Origin fixed, iteration # %s using %s elements:"%(nIter, np.shape(x)[0]))
                TEST = x[:, np.newaxis]
                slope, _, _, _ = np.linalg.lstsq(TEST, y)
                slope = slope[0]
                res = y - slope * x
                resMean = afwMath.makeStatistics(res, afwMath.MEANCLIP, sctrl).getValue()
                resStd = np.sqrt(afwMath.makeStatistics(res, afwMath.VARIANCECLIP, sctrl).getValue())
                # xxx check this line performs the same
                index = np.where((res > (resMean+nSigmaClip*resStd)) | (res < (resMean-nSigmaClip*resStd)))
                self.log.debug("%.3f %.3f %.3f %.3f"%(resMean, resStd, np.max(res), nSigmaClip))
                if np.shape(np.where(index))[1] == 0 or (nIter >= maxIter):  # run out of points or iters
                    break
                x = np.delete(x, index)
                y = np.delete(y, index)

            return slope, 0

        while nIter < maxIter:
            nIter += 1
            self.log.debug("Iteration # %s using %s elements:"%(nIter, np.shape(x)[0]))
            xx = np.vstack([x, np.ones(len(x))]).T
            ret, _, _, _ = np.linalg.lstsq(xx, y)
            slope, intercept = ret
            res = y - slope*x - intercept
            resMean = afwMath.makeStatistics(res, afwMath.MEANCLIP, sctrl).getValue()
            resStd = np.sqrt(afwMath.makeStatistics(res, afwMath.VARIANCECLIP, sctrl).getValue())
            index = np.where((res > (resMean + nSigmaClip * resStd)) | (res < resMean - nSigmaClip * resStd))
            self.log.debug("%.3f %.3f %.3f %.3f"%(resMean, resStd, np.max(res), nSigmaClip))
            if np.shape(np.where(index))[1] == 0 or (nIter >= maxIter):  # run out of points, or iterations
                break
            x = np.delete(x, index)
            y = np.delete(y, index)

        return slope, intercept

    def _generateKernel(self, corrs, means, rejectLevel=None):
        """Generate the full kernel from a list of (gain-corrected) cross-correlations and means.

        Taking a list of quarter-image, gain-corrected cross-correlations, do a pixel-wise sigma-clipped
        mean of each, and tile into the full-sized kernel image.

        Each corr in corrs is one quarter of the full cross-correlation, and has been gain-corrected.
        Each mean in means is a tuple of the means of the two individual images, corresponding to that corr.

        Parameters:
        -----------
        corrs : `list` of `numpy.ndarray`, (Ny, Nx)
            A list of the quarter-image cross-correlations
        means : `list` of `tuples` of `floats`
            The means of the input images for each corr in corrs
        rejectLevel : `float`, optional
            This is essentially is a sanity check parameter.
            If this condition is violated there is something unexpected going on in the image, and it is
            discarded from the stack before the clipped-mean is calculated.

        Returns:
        --------
        kernel : `numpy.ndarray`, (Ny, Nx)
            The output kernel
        """
        if not rejectLevel:
            rejectLevel = self.config.xcorrCheckRejectLevel

        if not isinstance(corrs, list):  # we expect a list of arrays
            corrs = [corrs]

        # Try to average over a set of possible inputs. This generates a simple function of the kernel that
        # should be constant across the images, and averages that.
        xcorrList = []
        sctrl = afwMath.StatisticsControl()
        sctrl.setNumSigmaClip(self.config.nSigmaClipKernelGen)

        for corrNum, ((mean1, mean2), corr) in enumerate(zip(means, corrs)):
            corr[0, 0] -= (mean1+mean2)
            if corr[0, 0] > 0:
                self.log.warn('Skipped item %s due to unexpected value of (variance-mean)!'%corrNum)
                continue
            corr /= -float(1.0*(mean1**2+mean2**2))

            fullCorr = self._tileArray(corr)

            # TODO: what is this block really testing? Is this what it should be doing? First line is fishy
            xcorrCheck = np.abs(np.sum(fullCorr))/np.sum(np.abs(fullCorr))
            if xcorrCheck > rejectLevel:
                self.log.warn("Sum of the xcorr is unexpectedly high. Investigate item num %s. \n"
                              "value = %s"%(corrNum, xcorrCheck))
                continue
            xcorrList.append(fullCorr)

        if not xcorrList:
            raise RuntimeError("Cannot generate kernel because all inputs were discarded. "
                               "Either the data is bad, or config.xcorrCheckRejectLevel is too low")

        # stack the individual xcorrs and apply a per-pixel clipped-mean
        meanXcorr = np.zeros_like(fullCorr)
        xcorrList = np.transpose(xcorrList)
        for i in range(np.shape(meanXcorr)[0]):
            for j in range(np.shape(meanXcorr)[1]):
                meanXcorr[i, j] = afwMath.makeStatistics(xcorrList[i, j], afwMath.MEANCLIP, sctrl).getValue()

        return self._SOR(meanXcorr)

    def _SOR(self, source, maxIter=None, eLevel=None):
        """An implementation of the successive over relaxation (SOR) method.

        Parameters:
        -----------
        source : `numpy.ndarray`, (Ny, Nx)
            The input array
        maxIter : `int`, optional
            Maximum number of iterations to attempt before aborting
        eLevel : `float`, optional
            The target error level factor at which we deem convergence to have occured

        Returns:
        --------
        output : `numpy.ndarray`, (Ny, Nx)
            The solution
        """
        if not maxIter:
            maxIter = self.config.maxIterSOR
        if not eLevel:
            eLevel = self.config.eLevelSOR

        # initialise, and set boundary conditions
        func = np.zeros([source.shape[0]+2, source.shape[1]+2])
        resid = np.zeros([source.shape[0]+2, source.shape[1]+2])
        rhoSpe = np.cos(np.pi/source.shape[0])  # Here a square grid is assummed

        inError = 0
        # Calculate the initial error
        for i in range(1, func.shape[0]-1):
            for j in range(1, func.shape[1]-1):
                resid[i, j] = (func[i, j-1]+func[i, j+1]+func[i-1, j] +
                               func[i+1, j]-4*func[i, j]-source[i-1, j-1])
        inError = np.sum(np.abs(resid))

        # Iterate until convergence
        # We perform two sweeps per cycle, updating 'odd' and 'even' points separately
        nIter = 0
        omega = 1.0
        dx = 1.0
        while nIter < maxIter*2:
            outError = 0
            if nIter%2 == 0:
                for i in range(1, func.shape[0]-1, 2):
                    for j in range(1, func.shape[0]-1, 2):
                        resid[i, j] = float(func[i, j-1]+func[i, j+1]+func[i-1, j] +
                                            func[i+1, j]-4.0*func[i, j]-dx*dx*source[i-1, j-1])
                        func[i, j] += omega*resid[i, j]*.25
                for i in range(2, func.shape[0]-1, 2):
                    for j in range(2, func.shape[0]-1, 2):
                        resid[i, j] = float(func[i, j-1]+func[i, j+1]+func[i-1, j] +
                                            func[i+1, j]-4.0*func[i, j]-dx*dx*source[i-1, j-1])
                        func[i, j] += omega*resid[i, j]*.25
            else:
                for i in range(1, func.shape[0]-1, 2):
                    for j in range(2, func.shape[0]-1, 2):
                        resid[i, j] = float(func[i, j-1]+func[i, j+1]+func[i-1, j] +
                                            func[i+1, j]-4.0*func[i, j]-dx*dx*source[i-1, j-1])
                        func[i, j] += omega*resid[i, j]*.25
                for i in range(2, func.shape[0]-1, 2):
                    for j in range(1, func.shape[0]-1, 2):
                        resid[i, j] = float(func[i, j-1]+func[i, j+1]+func[i-1, j] +
                                            func[i+1, j]-4.0*func[i, j]-dx*dx*source[i-1, j-1])
                        func[i, j] += omega*resid[i, j]*.25
            outError = np.sum(np.abs(resid))
            if outError < inError*eLevel:
                break
            if nIter == 0:
                omega = 1.0/(1-rhoSpe*rhoSpe/2.0)
            else:
                omega = 1.0/(1-rhoSpe*rhoSpe*omega/4.0)
            nIter += 1

        if nIter >= maxIter*2:
            self.log.warn("Did not converge in %s iterations.\noutError: %s, inError: "
                          "%s,"%(nIter//2, outError, inError*eLevel))
        else:
            self.log.info("Converged in %s iterations.\noutError: %s, inError: "
                          "%s", nIter//2, outError, inError*eLevel)
        return func[1:-1, 1:-1]

    # This sim code is used to estimate the bias correction used above.
    def xcorr_sim(self, im, im2, n=8, border=10, sigma=5):
        """Perform a simple xcorr from two images.

        It contains many elements of the actual code
        above (without individual amps and ISR removal )
        It takes two images, im and im2;
        n the max lag of the correlation function; border, the number of border
        pixels to discard; and sigma the sigma to use in the mean clip.
        """
        sctrl = afwMath.StatisticsControl()
        sctrl.setNumSigmaClip(sigma)
        im = self._convertImagelikeToFloatImage(im)
        im2 = self._convertImagelikeToFloatImage(im2)

        means1 = [0, 0]
        means1[0] = afwMath.makeStatistics(im[border:-border, border:-border],
                                           afwMath.MEANCLIP, sctrl).getValue()
        means1[1] = afwMath.makeStatistics(im2[border:-border, border:-border],
                                           afwMath.MEANCLIP, sctrl).getValue()
        im -= means1[0]
        im2 -= means1[1]
        diff = im2.clone()
        diff -= im.clone()
        diff = diff[border:-border, border:-border]
        binsize = self.config.backgroundBinSize
        nx = diff.getWidth()//binsize
        ny = diff.getHeight()//binsize
        bctrl = afwMath.BackgroundControl(nx, ny, sctrl, afwMath.MEANCLIP)
        bkgd = afwMath.makeBackground(diff, bctrl)
        diff -= bkgd.getImageF(afwMath.Interpolate.CUBIC_SPLINE, afwMath.REDUCE_INTERP_ORDER)
        dim0 = diff[0: -n, : -n].clone()
        dim0 -= afwMath.makeStatistics(dim0, afwMath.MEANCLIP, sctrl).getValue()
        w, h = dim0.getDimensions()
        xcorr = afwImage.ImageD(n + 1, n + 1)
        for di in range(n + 1):
            for dj in range(n + 1):
                dim_ij = diff[di:di + w, dj: dj + h].clone()
                dim_ij -= afwMath.makeStatistics(dim_ij, afwMath.MEANCLIP, sctrl).getValue()

                dim_ij *= dim0
                xcorr[di, dj] = afwMath.makeStatistics(dim_ij, afwMath.MEANCLIP, sctrl).getValue()
        L = np.shape(xcorr.getArray())[0]-1
        XCORR = np.zeros([2*L+1, 2*L+1])
        for i in range(L+1):
            for j in range(L+1):
                XCORR[i+L, j+L] = xcorr.getArray()[i, j]
                XCORR[-i+L, j+L] = xcorr.getArray()[i, j]
                XCORR[i+L, -j+L] = xcorr.getArray()[i, j]
                XCORR[-i+L, -j+L] = xcorr.getArray()[i, j]
        return (XCORR, xcorr, np.sum(means1), means1)

    def xcorr_bias(self, rangeMeans=[87500, 70000, 111000], repeats=5, sig=5,
                   border=3, seed=None, nx=2000, ny=4000, case=0, a=.1):
        """Fill images of specified size (nx and ny) with poisson points with means (in rangeMeans).

        before passing it to the above function with border and sig as above
        Repeats specifies the number of times to run the simulations.
        If case is 1 then a correlation between x_{i,j} and x_{i+1,j+1} is artificially introduced
        by adding a*x_{i,j} to x_{i+1,j+1}
        If seed is left to None the seed with be pulled from /dev/random.
        Else an int can be passed to see the random number generator.
        """
        if seed is None:
            with open("/dev/random", 'rb') as file:
                local_random = np.random.RandomState(int(file.read(4).encode('hex'), 16))
        else:
            local_random = np.random.RandomState(int(seed))
        MEANS = {}
        XCORRS = {}
        for M in rangeMeans:
            MEANS[M] = []
            XCORRS[M] = []

        if not case:
            for rep in range(repeats):
                for i, MEAN in enumerate(rangeMeans):

                    im = afwImage.ImageD(nx, ny)
                    im0 = afwImage.ImageD(nx, ny)
                    im.getArray()[:, :] = local_random.poisson(MEAN, (ny, nx))
                    im0.getArray()[:, :] = local_random.poisson(MEAN, (ny, nx))
                    XCORR, xcorr, means, MEANS1 = self.xcorr_sim(im, im0, border=border, sigma=sig)
                    MEANS[MEAN].append(means)
                    XCORRS[MEAN].append(xcorr)
                for i, MEAN in enumerate(rangeMeans):
                    self.log.debug("Simulated/Expected:", MEAN, MEANS[MEAN][-1],
                                   XCORRS[MEAN][-1].getArray()[0, 0]/MEANS[MEAN][-1])
        else:
            for rep in range(repeats):
                for i, MEAN in enumerate(rangeMeans):
                    im = afwImage.ImageD(nx, ny)
                    im0 = afwImage.ImageD(nx, ny)
                    im.getArray()[:, :] = local_random.poisson(MEAN, (ny, nx))
                    im.getArray()[1:, 1:] += a*im.getArray()[:-1, :-1]
                    im0.getArray()[:, :] = local_random.poisson(MEAN, (ny, nx))
                    im0.getArray()[1:, 1:] += a*im0.getArray()[:-1, :-1]
                    XCORR, xcorr, means, MEANS1 = self.xcorr_sim(im, im0, border=border, sigma=sig)
                    MEANS[MEAN].append(means)
                    XCORRS[MEAN].append(xcorr)
                for i, MEAN in enumerate(rangeMeans):
                    self.log.debug("Simulated/Expected:", MEANS[MEAN][-1], '\n',
                                   (XCORRS[MEAN][-1].getArray()[1, 1]/MEANS[MEAN][-1]*(1+a))/.1)
        return MEANS, XCORRS

    @staticmethod
    def _tileArray(in_array):
        """Given a square input quarter-image, tile/mirror it, returning the full image.

        Given an input of side-length n, of the form

        input = array([[1, 2, 3],
                       [4, 5, 6],
                       [7, 8, 9]])

        return an array of size 2n-1 as

        output = array([[ 9,  8,  7,  8,  9],
                        [ 6,  5,  4,  5,  6],
                        [ 3,  2,  1,  2,  3],
                        [ 6,  5,  4,  5,  6],
                        [ 9,  8,  7,  8,  9]])

        Parameters:
        -----------
        input : `np.array`
            The square input quarter-array

        Returns:
        --------
        output : `np.array`
            The full, tiled array
        """
        assert(in_array.shape[0] == in_array.shape[1])
        length = in_array.shape[0]-1
        output = np.zeros((2*length+1, 2*length+1))

        for i in range(length+1):
            for j in range(length+1):
                output[i+length, j+length] = in_array[i, j]
                output[-i+length, j+length] = in_array[i, j]
                output[i+length, -j+length] = in_array[i, j]
                output[-i+length, -j+length] = in_array[i, j]
        return output

    @staticmethod
    def _convertImagelikeToFloatImage(imagelikeObject):
        """Turn an exposure or masked image of any type into an ImageF."""
        for attr in ("getMaskedImage", "getImage"):
            if hasattr(imagelikeObject, attr):
                imagelikeObject = getattr(imagelikeObject, attr)()
        try:
            floatImage = imagelikeObject.convertF()
        except AttributeError:
            raise RuntimeError("Failed to convert image to float")
        return floatImage

    # def _xcorr(self, im1, im2, gains):
    #     """Calculate the cross-correlation of two images im1 and im2 using robust measures of the covariance.

    #     Parameters:
    #     -----------
    #     im1 : `afwImage.Image` or similar
    #         The first image-like object, to be cross-correlated with the second
    #     im2 : `afwImage.Image` or similar
    #         The second image-like object, to be cross-correlated with the first
    #     gains : `list` of `float`
    #         The per-amplifier gains for the detector

    #     Returns:
    #     --------
    #     xcorr : `np.array`
    #         The quarter-image cross-corellation
    #     means : `list` of `float`
    #         The as-calculated means of the input images (clipped, and with borders applied)

    #     Notes:
    #     ------
    #     This function is controlled by the following pexConfig parameters:

    #     maxLag : `int`
    #         The maximum lag to use in the cross-correlation calculation
    #     nPixBorderXCorr : `int`
    #         The number of border pixels to exclude
    #     nSigmaClipXCorr : `float`
    #         The number of sigma to be clipped to
    #     biasCorr : `float`
    #         Parameter used to correct from the bias introduced by the sigma cuts.
    #     """
    #     maxLag = self.config.maxLag
    #     border = self.config.nPixBorderXCorr
    #     sigma = self.config.nSigmaClipXCorr
    #     biasCorr = self.config.biasCorr

    #     sctrl = afwMath.StatisticsControl()
    #     sctrl.setNumSigmaClip(sigma)

    #     means = [None, None]
    #     means1 = [None, None]
    #     for imNum, im in enumerate([im1, im2]):
    #         ccd = im.getDetector()
    #         #
    #         # Starting with an Exposure, MaskedImage, or Image trim the data and convert to float
    #         #
    #         for attr in ("getMaskedImage", "getImage"):
    #             if hasattr(im, attr):
    #                 im = getattr(im, attr)()
    #         try:
    #             im = im.convertF()
    #         except AttributeError:
    #             pass
    #         # im = trim(im, ccd)
    #         means[imNum] = afwMath.makeStatistics(im[border:-border, border:-border],
    #                                               afwMath.MEANCLIP, sctrl).getValue()
    #         temp = im.clone()
    #         # Rescale each amp by the appropriate gain and subtract the mean.
    #         for ampNum, amp in enumerate(ccd):
    #             # smi = im[amp.getDataSec(True)]
    #             # smiTemp = temp[amp.getDataSec(True)]
    #             smi = im[amp.getBBox()]
    #             smiTemp = temp[amp.getBBox()]
    #             mean = afwMath.makeStatistics(smi, afwMath.MEANCLIP, sctrl).getValue()
    #             gain = gains[ampNum]
    #             smi *= gain
    #             self.log.debug(mean*gain, afwMath.makeStatistics(smi, afwMath.MEANCLIP, sctrl).getValue())
    #             smi -= mean*gain
    #             smiTemp *= gain
    #         means1[imNum] = afwMath.makeStatistics(temp[border:-border, border:-border],
    #                                                afwMath.MEANCLIP, sctrl).getValue()
    #         self.log.debug(afwMath.makeStatistics(temp[border:-border, border:-border],
    #                                               afwMath.MEANCLIP, sctrl).getValue())
    #     #
    #     # Actually diff the images
    #     #
    #     diff = im1.clone()
    #     diff = diff.getMaskedImage().getImage()
    #     diff -= im2.getMaskedImage().getImage()

    #     diff = diff[border:-border, border:-border]
    #     # diff.writeFits("./Data/Diff_CCD_"+str(CCD)+".fits")
    #     #
    #     # Subtract background.  It should be a constant, but it isn't always
    #     #
    #     binsize = self.config.backgroundBinSize
    #     nx = diff.getWidth()//binsize
    #     ny = diff.getHeight()//binsize
    #     bctrl = afwMath.BackgroundControl(nx, ny, sctrl, afwMath.MEANCLIP)
    #     bkgd = afwMath.makeBackground(diff, bctrl)
    #     diff -= bkgd.getImageF(afwMath.Interpolate.CUBIC_SPLINE, afwMath.REDUCE_INTERP_ORDER)

    #     if self.debug.display:
    #         try:
    #             frameId1 = im1.getMetadata().get("FRAMEID")
    #             frameId2 = im2.getMetadata().get("FRAMEID")
    #             frameId = ' diff '.join(frameId1, frameId2)
    #         except:
    #             frameId = 'Im1 diff Im2'
    #         self.disp1.mtv(diff, title=frameId)

    #     if False:  # xxx work out what this was ever about
    #         global diffim
    #         diffim = diff
    #     self.log.debug("Median and variance of diff:")
    #     self.log.debug(afwMath.makeStatistics(diff, afwMath.MEDIAN, sctrl).getValue())
    #     self.log.debug(afwMath.makeStatistics(diff, afwMath.VARIANCECLIP,
    #                                           sctrl).getValue(), np.var(diff.getArray()))
    #     #
    #     # Measure the correlations
    #     #
    #     dim0 = diff[0: -maxLag, : -maxLag]
    #     dim0 -= afwMath.makeStatistics(dim0, afwMath.MEANCLIP, sctrl).getValue()
    #     w, h = dim0.getDimensions()
    #     xcorr = np.zeros((maxLag + 1, maxLag + 1), dtype=np.float64)

    #     for xlag in range(maxLag + 1):
    #         for ylag in range(maxLag + 1):
    #             dim_xy = diff[xlag:xlag + w, ylag: ylag + h].clone()
    #             dim_xy -= afwMath.makeStatistics(dim_xy, afwMath.MEANCLIP, sctrl).getValue()
    #             dim_xy *= dim0
    #             xcorr[xlag, ylag] = afwMath.makeStatistics(dim_xy,
    #                                                        afwMath.MEANCLIP, sctrl).getValue()/(biasCorr)

    #     xcorr_full = self._tileArray(xcorr)
    #     self.log.debug(sum(means1), xcorr[0, 0], np.sum(xcorr_full), xcorr[0, 0]/sum(means1),
    #                    np.sum(xcorr_full)/sum(means1))
    #     return (xcorr, means1)
