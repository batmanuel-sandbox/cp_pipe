#
# LSST Data Management System
#
# Copyright 2008-2017  AURA/LSST.
#
# This product includes software developed by the
# LSST Project (http://www.lsst.org/).
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the LSST License Statement and
# the GNU General Public License along with this program.  If not,
# see <https://www.lsstcorp.org/LegalNotices/>.
#

"""Calculation of brighter-fatter effect correlations and kernels."""
from __future__ import print_function

from builtins import zip
from builtins import str
from builtins import range
import os
import re
import pickle
from scipy import stats
# import matplotlib as mpl
import numpy as np
# mpl.use('Agg')
# pyplot = plt
import matplotlib.pyplot as plt

import lsst.afw.image as afwImage
import lsst.afw.math as afwMath
import lsst.afw.display.ds9 as ds9
import lsst.pex.config as pexConfig
import lsst.pipe.base as pipeBase
# import lsst.log as lsstLog
from lsst.obs.subaru.crosstalk import CrosstalkTask
from lsst.obs.subaru.isr import SubaruIsrTask

import lsstDebug
debug = lsstDebug.Info(__name__)

# try:
#     import scipy
#     import scipy.interpolate
# except ImportError:
#     scipy = None

OUTPUT_PATH = '/home/mfl/bf_testing/'


class BfTaskConfig(pexConfig.Config):
    """Config class for bright-fatter effect coefficient calculation."""

    doCalcGains = pexConfig.Field(
        dtype=bool,
        doc="Measure the per-amplifier gains using the PTC method",
        default=True,
    )
    maxIterRegression = pexConfig.Field(
        dtype=int,
        doc="Maximum number of iterations for the iterative regression fitter",
        default=10
    )
    nSigmaClipRegression = pexConfig.Field(
        dtype=int,
        doc="Number of sigma to clip to during iterative regression",
        default=3
    )
    xcorrCheckRejectLevel = pexConfig.Field(
        dtype=float,
        doc="Sanity check level for the sum of the input cross-correlations. Arrays which "
        "sum to greater than this are discarded before the clipped mean is calculated.",
        default=1000.0  # xxx change this back once the problem is fixed!
        # default=0.2
    )
    maxIterSOR = pexConfig.Field(
        dtype=int,
        doc="The maximum number of iterations allowed for the successive over-relaxation method",
        default=10000
    )
    eLevelSOR = pexConfig.Field(
        dtype=float,
        doc="The target residual error for the successive over-relaxation method",
        default=5.0e-14
    )
    nSigmaClipKernelGen = pexConfig.Field(
        dtype=float,
        doc="Number of sigma to clip to during pixel-wise clipping when generating the kernel",
        default=4
    )
    nSigmaClipXCorr = pexConfig.Field(
        dtype=float,
        doc="Number of sigma to clip when calculating means for the cross correlation",
        default=5
    )
    maxLag = pexConfig.Field(
        dtype=int,
        doc="The maximum lag to use when calculating the cross correlation/kernel",
        default=5
    )
    nPixBorderXCorr = pexConfig.Field(
        dtype=int,
        doc="The number of border pixels to exclude when calculating the cross correlation/kernel",
        default=10
    )
    biasCorr = pexConfig.Field(
        dtype=float,
        doc="A scary, empirically determined correction-factor, correcting for the sigma-clipping " + 
        "a non-Gaussian distribution",
        default=0.9241
    )


class BfTaskRunner(pipeBase.TaskRunner):
    """Subclass of TaskRunner for the bfTask.

    bfTask.run() takes a two arguments...
    xxx complete this
    of arguments, one of which is a list of dataRefs
    extracted from the command line (whereas most CmdLineTasks' run methods take a
    single dataRef, are are called repeatedly). This class transforms the processed
    arguments generated by the ArgumentParser into the arguments expected by
    bfTask.run().

    See pipeBase.TaskRunner for more information.
    """

    @staticmethod
    def getTargetList(parsedCmd, **kwargs):
        """Pass the visit list through explicitly."""
        visitPairs = parsedCmd.visitPairs

        tuples = visitPairs.split("),(")  # split
        tuples[0] = tuples[0][1:]  # remove leading "(
        tuples[-1] = tuples[-1][:-1]  # remove trailing )"
        visitPairs = [(int(v1), int(v2)) for (v1, v2) in [tup.split(',') for tup in tuples]]  # cast to int

        # if the reviewer is a die-hard regex fan then uncomment the below, but the above is more readable
        # visitPairs = re.findall(r'\((\d+),\s*(\d+)\)', visitPairs)  # break down visit pair list
        # visitPairs = [tuple([int(y) for y in x]) for x in visitPairs]  # make a tuple of ints
        return pipeBase.TaskRunner.getTargetList(parsedCmd, visitPairs=visitPairs, **kwargs)


class BfDataIdContainer(pipeBase.DataIdContainer):
    """A DataIdContainer for the BF task."""

    def makeDataRefList(self, namespace):
        """Compute refList based on idList.

        Parameters
        ----------
        namespace
            Results of parsing command-line (with ``butler`` and ``log`` elements).

        Notes
        -----
        Not called if ``add_id_argument`` called with ``doMakeDataRefList=False``.
        Note that this is almost a copy-and-paste of the vanilla implementation, but without checking
        if the datasets already exist as this task exists to make them.
        """
        if self.datasetType is None:
            raise RuntimeError("Must call setDatasetType first")
        butler = namespace.butler
        for dataId in self.idList:
            refList = list(butler.subset(datasetType=self.datasetType, level=self.level, dataId=dataId))
            # exclude nonexistent data
            # this is a recursive test, e.g. for the sake of "raw" data
            if not refList:
                namespace.log.warn("No data found for dataId=%s", dataId)
                continue
            self.refList += refList


class BfTask(pipeBase.CmdLineTask):
    """Bright-fatter effect coefficient calculation task.

    See http://ls.st/ldm-151 Chapter 4, Calibration Products Production for further details
    regarding the inputs and outputs.
    """

    RunnerClass = BfTaskRunner
    ConfigClass = BfTaskConfig
    _DefaultName = "bf"

    def __init__(self, *args, **kwargs):
        """Constructor for the BfTask."""
        pipeBase.CmdLineTask.__init__(self, *args, **kwargs)

        self.config.validate()
        self.config.freeze()

    @classmethod
    def _makeArgumentParser(cls):
        """Augment argument parser to xxx."""
        parser = pipeBase.ArgumentParser(name=cls._DefaultName)
        parser.add_argument("--visitPairs", help="The list of visit pairs to use, as a list of tuples "
                            "enclosed in quotes e.g. \"(123,456),(789,987),(654,321)\""
                            "NB: must be comma-separated-tuples with no spaces, enclosed in quotes!")
        parser.add_id_argument("--id", datasetType="bfKernelNew", ContainerClass=BfDataIdContainer,
                               help="The ccds to use, e.g. --id ccd=0..100")
        return parser

    # @pipeBase.timeMethod  # xxx
    def run(self, dataRef, visitPairs):
        """Run the brighter-fatter measurement task.

        For a dataRef (which is each ccd here), and given a list of visit pairs, calulate the
        brighter-fatter kernel for the ccd.

        TODO: might want to have some syntax for ganging ccds together
        #    though this might just be all-together, or all separate actually
        #    it's possible we could gang together on boule/batch, but let's not worry for now

        Parameters
        ----------
        dataRef : list of lsst.daf.persistence.ButlerDataRef
            dataRef for the CCD for the visits to be fit.
        visitPairs : `iterable` of `tuple` of `int`
            Pairs of visit numbers to be processed together
        """

        # self.xxx_test_estimateGains()
        # self.xxx_test_generateKernel()
        # self.xxx_test_xcorr()
        # self.xxx_test_put(dataRef)
        # return

        gains = []
        xcorrs = []
        means = []

        ccdNum = dataRef.dataId['ccd']

        if self.config.doCalcGains:
            self.log.info('Beginning gain estimation for CCD %s'%ccdNum)
            gains, nomGains = self.estimateGains(dataRef, visitPairs)
            dataRef.put(gains, datasetType='bfGain')
            self.log.info('Finished gain estimation for CCD %s'%ccdNum)
        else:
            gains = dataRef.get('bfGain')
            if not gains:
                self.log.fatal('Failed to retrieved gains for CCD %s'%ccdNum)
                raise RuntimeError("Must either calculate or supply gains for %s"%ccdNum)
            self.log.info('Retrieved stored gain for CCD %s'%ccdNum)
        self.log.debug('CCD %s has gains %s'%(ccdNum, gains))

        # calculating the cross correlations
        for (v1, v2) in visitPairs:
            xcorr, mean = self.xcorrFromVisitPair(dataRef, v1, v2, gains=gains)
            xcorrs.append(xcorr)
            means.append(mean)

        kernel = self._generateKernel(xcorrs, means)
        dataRef.put(kernel)

        # for (v1, v2) in visitPairs:
        #     im1 = dataRef.get('raw', visit=v1)
        #     # im2 = dataRef.get('raw', dataId={'ccd': dataRef.dataId, 'visit': v2})
        #     break

        print('finished')

        return pipeBase.Struct(exitStatus=0)

    def xcorrFromVisitPair(self, dataRef, v1, v2, gains):
        """Return the cross-correlation from a given pair of visits.

        This is code preforms some preliminary operations and then calls the main correlation calc code.
        This is used for calculating the xcorr after setting the gains.

        Parameters:
        -----------
        dataRef : list of lsst.daf.persistence.ButlerDataRef
            dataRef for the CCD for the visits to be fit.
        v1 : `int`
            Visit number of the first visit
        v2 : `int`
            Visit number of the second visit
        gains : `iterable` of `float`
            The gains for each amplifier in the CCD

        Returns:
        xcorr : `np.array`
            The cross-correlation numpy array
        means : `list` of `float`
            The sigma-clipped-mean flux in the input images
        """
        im1 = self.isr(dataRef, v1)
        im2 = self.isr(dataRef, v2)
        xcorr, xcorrMeans = self._xcorr(im1, im2, gains)

        # TODO: Change to lsstDebug
        if False:
            means = [afwMath.makeStatistics(im, afwMath.MEANCLIP).getValue() for im in [im1, im2]]
            self._plotXcorr(xcorr.clone(), (xcorrMeans[0]+means[1]),
                            title=r"Visits %s; %s, CCDs %s  $\langle{I}\rangle"
                            " = %.3f$ (%s) Var = %.4f" %
                            (self._getNameOfSet(v1), self._getNameOfSet(v2), self._getNameOfSet(ccds),
                             (xcorrMeans[0]+means[1]), im1.getFilter().getName(), float(xcorr[0, 0]) /
                             (xcorrMeans[0]+means[1])), zmax=zmax, fig=fig, SAVE=True,
                            fileName=(os.path.join(OUTPUT_PATH, ("Xcorr_visit_" + str(v1[0])+"_"+str(v2[0])+"_ccd_" +
                                                                 str(ccds[0])+".png"))))
        return xcorr, xcorrMeans

    def xxx_test_put(self, dataRef):
        """Xxx Docstring."""
        from time import sleep
        print('Starting, if parallelised this will be printed -j times simultaneously')
        sleep(4)
        print('finished')
        a = np.zeros((3, 3))
        dataRef.put(a)
        return

    def xxx_test_xcorr(self):
        """Xxx Docstring."""
        import lsst.daf.persistence as dafPersist
        butler = dafPersist.Butler('/datasets/hsc/repo/')
        visPairs = [(904606, 904608),
                    (904610, 904612)]
        xcorrDict = {}

        ignoreCcdList = [_ for _ in range(112)]
        ignoreCcdList.remove(41)

        for visPair in visPairs():

            # xcorrDict...
            self.xxx(butler, visPairs, ignoreCcdList)

        return

    def xxx_test_estimateGains(self):
        """Xxx Docstring."""
        import lsst.daf.persistence as dafPersist
        butler = dafPersist.Butler('/datasets/hsc/repo/')
        visPairs = [(904606, 904608),
                    (904610, 904612)]
        ignoreCcdList = [_ for _ in range(112)]
        ignoreCcdList.remove(41)
        self.estimateGains(butler, visPairs, ignoreCcdList)

    def xxx_test_generateKernel(self):
        """Docstring."""
        import pickle
        f = open('/home/mfl/bf_output/merlinTestXcorr.pkl', 'rb')
        xcorr, means = pickle.load(f)
        f.close()
        print('\n\n Level = %s\n\n'%self.config.xcorrCheckRejectLevel)
        kernel = self._generateKernel(xcorr, means)
        f = open('/home/mfl/bf_output/taskOutput_kernel.pkl', 'wb')
        pickle.dump(kernel, f)
        f.close()

    def estimateGains(self, dataRef, visitPairs, intercept=0, writeGains=True,
                      xxx_figLocation=OUTPUT_PATH, xxx_plot=False):
        """Estimate the gains of the amplifiers in the CCD using the specified visits.

        Given a dataRef and list of flats of varying intensity, calculate the gain for each
        CCD specified using the PTC method.

        The intercept option chooses the linear fitting option. The default fits
        Var=1/g mean, if non zero Var=1/g mean + const is fit.
        By default, gains are persisted per-amplifier as a dictionary

        XXX This is really a ptcGainTask by Will. Should this move to its own task?
        TODO: compare results from this task to the eotest PTC gain task once that's ported

        Parameters
        ----------
        dataRef : `lsst.daf.persistence.butler.Butler.dataRef`
            dataRef for the CCD for the flats to be used
        visitPairs : `list` of `tuple`
            List of visit-pairs to use, as [(v1,v2), (v3,v4)...]
        writeGains : `bool`
            Persist the calculated gain values

        Returns
        -------
        gains : `dict`
            Amplifier gain values, as calculated
        nominalGains : `dict`
            Amplifier gains, as given by the `detector` objects
        """
        ampMeans = []
        ampVariances = []
        ampCorrVariances = []
        ampGains = []
        nomGains = []

        # Loop over the amps in the CCD, calculating a PTC for each amplifier.
        # The amplifier iteration is performed in _calcMeansAndVars()
        # NB: no gain correction is applied
        for visPairNum, visPair in enumerate(visitPairs):
            _means, _vars, _covars, _gains = self._calcMeansAndVars(dataRef, visPair[0], visPair[1])
            breaker = 0
            # Do sanity checks; if these are failed more investigation is needed!
            for i, j in enumerate(_means):
                if _means[i]*10 < _vars[i] or _means[i]*10 < _covars[i]:
                    self.log.warn('Sanity check failed; check visit %s'%visPair)
                    breaker += 1
            if breaker:
                continue
            if visPairNum == 0:
                for i in range(len(_means)):
                    ampMeans.append(np.array([]))
                    ampVariances.append(np.array([]))
                    ampCorrVariances.append(np.array([]))
                    ampGains.append(np.array([]))
            for i, j in enumerate(_means):
                if visPairNum == 0:
                    nomGains.append(_gains[i])
                if _vars[i]*1.3 < _covars[i] or _vars[i]*0.7 > _covars[i]:
                    continue
                ampMeans[i] = np.append(ampMeans[i], _means[i])
                ampVariances[i] = np.append(ampVariances[i], _vars[i])
                ampCorrVariances[i] = np.append(ampCorrVariances[i], _covars[i])
                ampGains[i] = np.append(ampGains[i], _gains[i])

        # TODO: Change the "intercept" option to a pexConfig option (or decide which is best and remove)
        fig = None
        gains = []
        for i in range(len(ampMeans)):
            slope2, intercept, r_value, p_value, std_err = stats.linregress(ampMeans[i], ampCorrVariances[i])
            slope, _ = self.iterativeRegression(ampMeans[i], ampCorrVariances[i], fixThroughOrigin=True)
            slope3, intercept2 = self.iterativeRegression(ampMeans[i], ampCorrVariances[i])
            # TODO: Change messages to say what these ARE, not just second/third fits
            self.log.info("slope of fit: %s intercept of fit: %s p value: %s"%(slope2, intercept, p_value))
            self.log.info("slope of second fit: %s, difference:%s"%(slope, slope-slope2))
            self.log.info("slope of third  fit: %s, difference:%s"%(slope3, slope-slope3))
            if intercept:
                slope = slope3

            # TODO: replace with lsstDebug
            if xxx_plot:  # TODO: replace with lsstDebug.Also, consider dumping based on p_value or X_sq?
                if fig is None:
                    fig = plt.figure()
                else:
                    fig.clf()
                ax = fig.add_subplot(111)
                ax.plot(ampMeans[i], ampCorrVariances[i], linestyle='None', marker='x', label='data')
                if intercept:
                    ax.plot(ampMeans[i], ampMeans[i]*slope+intercept2, label='fix')
                else:
                    ax.plot(ampMeans[i], ampMeans[i]*slope, label='fix')
                ccdNum = dataRef.dataId['ccd']
                fig.savefig(os.path.join(xxx_figLocation, ('PTC_CCD_'+str(ccdNum)+'_AMP_'+str(i)+'.pdf')))
            gains.append(1.0/slope)
        return gains, nomGains

    def _calcMeansAndVars(self, dataRef, v1, v2, n=8, border=10, plot=False, zmax=.05,
                          fig=None, display=False, sigma=5, biasCorr=0.9241):
        """Calculate the means, vars, covars, and retieve the nominal gains, for each amp in each ccd.

        This code runs using two visit numbers, and for ccd specified.
        It calculates the correlations in the individual amps without rescaling any gains.
        This allows a photon transfer curve to be generated and the gains measured.

        Images are assembled with use the isrTask, and basic isr is performed.
        Note that the isr task used MUST set the EDGE bits.[xxx need to change to using this, or change this]

        Parameters:
        -----------
        dataRef : `lsst.daf.persistence.butler.Butler.dataRef`
            Butler for the repo containg the flats to be used
        v1 : `int`
            First visit of the visit pair
        v2 : `int`
            Second visit of the visit pair
        ccd : `string` or `int`
            Names of the ccds to use

        Returns
        -------
        means, vars, covars, gains : `tuple` of `lists`
            The sum of the means, variance, one quarter of the xcorr, and the original gain for each amp.
        """
        nomGains = []
        imMeans = [None, None]
        ampMeans = [[], []]

        # TODO_URGENT: turn this into a dict so that we don't get muddled up. Currently this is nonsense.
        # TODO: change to looping over ccds so that we don't hold all the isr-ed images

        # TODO: reinclude the ISR step at this point
        ims = [self.isr(dataRef, v1), self.isr(dataRef, v2)]
        # ims = [dataRef.get('raw', visit=v1), dataRef.get('raw', visit=v2)]
        # if d1isplay:  # TODO: replace with lsstDebug
        #     ds9.mtv(trim(ims[i]), frame=i, title=v)

        sctrl = afwMath.StatisticsControl()
        sctrl.setNumSigmaClip(sigma)  # TODO: change to pexConfig option
        for imNum, im in enumerate(ims):
            ccd = im.getDetector()
            # Starting with an Exposure, MaskedImage, or Image trim the data and convert to float
            for attr in ("getMaskedImage", "getImage"):
                if hasattr(im, attr):
                    im = getattr(im, attr)()
            try:
                im = im.convertF()
            except AttributeError:
                self.log.warn("Failed to convert image %s to float"%imNum)  # xxx fatal? Raise?
                pass

            # calculate the sigma-clipped mean, excluding the borders
            # TODO: rewrite to use egde bits
            imMeans[imNum] = afwMath.makeStatistics(im, afwMath.MEANCLIP, sctrl).getValue()
            for ampNum, amp in enumerate(ccd):
                ampIm = im[amp.getBBox()]
                if ampNum == 0:
                    mean = afwMath.makeStatistics(ampIm[border:, border:-border],
                                                  afwMath.MEANCLIP).getValue()
                elif ampNum == 3:
                    mean = afwMath.makeStatistics(ampIm[:-border, border:-border],
                                                  afwMath.MEANCLIP).getValue()
                else:
                    mean = afwMath.makeStatistics(ampIm[:, border:-border], afwMath.MEANCLIP).getValue()
                nomGain = amp.getGain()
                ampMeans[imNum].append(mean)
                if imNum == 0:
                    nomGains.append(nomGain)
                ampIm -= mean

        diff = ims[0].clone()
        diff = diff.getMaskedImage().getImage()
        diff -= ims[1].getMaskedImage().getImage()

        temp = diff[border:-border, border:-border]

        # Subtract background.  It should be a constant, but it isn't always (e.g. some SuprimeCam flats)
        # TODO: Check how this looks, and if this is the "right" way to do this
        binsize = 128  # TODO: change to pexConfig option
        nx = temp.getWidth()//binsize
        ny = temp.getHeight()//binsize
        bctrl = afwMath.BackgroundControl(nx, ny, sctrl, afwMath.MEANCLIP)
        bkgd = afwMath.makeBackground(temp, bctrl)
        diff[border:-border, border:-border] -= bkgd.getImageF(afwMath.Interpolate.CUBIC_SPLINE,
                                                               afwMath.REDUCE_INTERP_ORDER)
        variances = []  # can't shadow builtin "vars"
        coVars = []
        # For each amp calculate the correlation
        # xxx can you do this for a heterogenous focal plane? (answer: 100% no)
        # xxx update note to self - now that we're doing ccd by ccd with a dataRef this is fine again
        CCD = ims[0].getDetector()
        for ampNum, amp in enumerate(CCD):
            borderL = 0
            borderR = 0
            if ampNum == 0:  # TODO: this needs rewriting for using edge bits to make camera agnostic
                borderL = border
            if ampNum == 3:
                borderR = border

            diffAmpIm = diff[amp.getBBox()].clone()  # xxx why is this a clone? move .clone() to next line?
            diffAmpImCrop = diffAmpIm[borderL:-borderR-n, border:-border-n]
            diffAmpImCrop -= afwMath.makeStatistics(diffAmpImCrop, afwMath.MEANCLIP, sctrl).getValue()
            w, h = diffAmpImCrop.getDimensions()
            xcorr = np.zeros((n + 1, n + 1), dtype=np.float64)

            # calculate the cross correlation
            for xlag in range(n + 1):
                for ylag in range(n + 1):
                    dim_xy = diffAmpIm[borderL+xlag:borderL+xlag + w, border+ylag: border+ylag + h].clone()
                    dim_xy -= afwMath.makeStatistics(dim_xy, afwMath.MEANCLIP, sctrl).getValue()
                    dim_xy *= diffAmpImCrop
                    xcorr[xlag, ylag] = afwMath.makeStatistics(dim_xy,
                                                               afwMath.MEANCLIP, sctrl).getValue()/(biasCorr)

            variances.append(xcorr[0, 0])
            xcorr_full = self._tileArray(xcorr)
            coVars.append(np.sum(xcorr_full))

            msg = "M1: " + str(ampMeans[0][ampNum])
            msg += " M2 " + str(ampMeans[1][ampNum])
            msg += " M_sum: " + str((ampMeans[0][ampNum])+ampMeans[1][ampNum])
            msg += " Var " + str(variances[ampNum])
            msg += " coVar: " + str(coVars[ampNum])
            self.log.info(msg)  # xxx change to debug or trace level
        return ([i+j for i, j in zip(ampMeans[1], ampMeans[0])], variances, coVars, nomGains)

    def isr(self, dataRef, visit):  # TODO: Need to replace this with a retargetable ISR task
        """Some simple code to perform some simple ISR."""
        dataRef.dataId['visit'] = visit
        config = SubaruIsrTask.ConfigClass()
        # config.load(os.path.join(os.environ["OBS_SUBARU_DIR"], "config", "isr.py"))
        # config.load(os.path.join(os.environ["OBS_SUBARU_DIR"], "config", "hsc", "isr.py"))

        config.doFlat = False
        config.doGuider = False
        config.doSaturation = True
        config.doWrite = False
        config.doDefect = True
        config.qa.doThumbnailOss = False
        config.qa.doThumbnailFlattened = False
        config.doFringe = False
        config.fringe.filters = ['y', ]
        config.overscanFitType = "AKIMA_SPLINE"
        config.overscanOrder = 30
        config.doAttachTransmissionCurve = False
        # Overscan is fairly efficient at removing bias level, but leaves a line in the middle
        config.doBias = True
        config.doDark = True  # Required especially around CCD 33
        config.crosstalk.retarget(CrosstalkTask)
        config.crosstalk.value.coeffs.values = [0.0e-6, -125.0e-6, -149.0e-6, -156.0e-6, -124.0e-6, 0.0e-6, -
                                                132.0e-6, -157.0e-6, -171.0e-6, -134.0e-6, 0.0e-6, -153.0e-6,
                                                -157.0e-6, -151.0e-6, -137.0e-6, 0.0e-6, ]
        isr = SubaruIsrTask(config=config)
        exp = isr.run(dataRef).exposure
        return exp

    def plotXcorr(self, xcorr, mean, zmax=0.05, title=None, fig=None, SAVE=False, fileName=None):
        """This program is used to plot the correlation functions."""
        try:
            xcorr = xcorr.getArray()
        except:
            pass

        xcorr /= float(mean)
        # xcorr.getArray()[0,0]=abs(xcorr.getArray()[0,0]-1)

        if fig is None:
            fig = plt.figure()
        else:
            fig.clf()

        ax = fig.add_subplot(111, projection='3d')
        ax.azim = 30
        ax.elev = 20

        nx, ny = np.shape(xcorr)

        xpos, ypos = np.meshgrid(np.arange(nx), np.arange(ny))
        xpos = xpos.flatten()
        ypos = ypos.flatten()
        zpos = np.zeros(nx*ny)
        dz = xcorr.flatten()
        dz[dz > zmax] = zmax

        ax.bar3d(xpos, ypos, zpos, 1, 1, dz, color='b', zsort='max', sort_zpos=100)
        if xcorr[0, 0] > zmax:
            ax.bar3d([0], [0], [zmax], 1, 1, 1e-4, color='c')

        ax.set_xlabel("row")
        ax.set_ylabel("column")
        ax.set_zlabel(r"$\langle{(F_i - \bar{F})(F_i - \bar{F})}\rangle/\bar{F}$")

        if title:
            fig.suptitle(title)
        if SAVE == True:
            fig.savefig(fileName)
        # plt.close(fig)
        return fig, ax

    @staticmethod
    def _getNameOfSet(vals):
        """Convert a list of numbers into a string, merging consecutive values."""
        if not vals:
            return ""

        def _addPairToName(valName, val0, val1):
            """Add a pair of values, val0 and val1, to the valName list."""
            sval1 = str(val1)
            if val0 != val1:
                pre = os.path.commonprefix([str(val0), sval1])
                sval1 = int(sval1[len(pre):])
            valName.append("%s-%s" % (val0, sval1) if val1 != val0 else str(val0))

        valName = []
        val0 = vals[0]
        val1 = val0
        for val in vals[1:]:
            if isinstance(val, int) and val == val1 + 1:
                val1 = val
            else:
                _addPairToName(valName, val0, val1)
                val0 = val
                val1 = val0

        _addPairToName(valName, val0, val1)

        return ", ".join(valName)

    def iterativeRegression(self, x, y, fixThroughOrigin=False, nSigmaClip=None, maxIter=None):
        """Use linear regression to fit a line of best fit, iteratively removing outliers.

        Useful when you have sufficiently large numbers of points on your PTC.
        Function iterates until either there are no outliers of nSigmaClip magnitude, or until the specified
        max number of iterations have been performed.

        Parameters:
        -----------
        x : `numpy.array`
            The independent variable
        y : `numpy.array`
            The dependent variable

        Returns:
        --------
        slope : `float`
            The slope of the line of best fit
        intercept : `float`
            The y-intercept of the line of best fit
        """
        if not maxIter:
            maxIter = self.config.maxIterRegression
        if not nSigmaClip:
            nSigmaClip = self.config.nSigmaClipRegression

        nIter = 0
        sctrl = afwMath.StatisticsControl()
        sctrl.setNumSigmaClip(nSigmaClip)

        if fixThroughOrigin:
            while nIter < maxIter:  # TODO: change log levels to debug
                nIter += 1
                self.log.info("Origin fixed, iteration # %s, %s elements:"%(nIter, np.shape(x)[0]))
                TEST = x[:, np.newaxis]
                slope, _, _, _ = np.linalg.lstsq(TEST, y)
                slope = slope[0]
                res = y - slope * x
                resMean = afwMath.makeStatistics(res, afwMath.MEANCLIP, sctrl).getValue()
                resStd = np.sqrt(afwMath.makeStatistics(res, afwMath.VARIANCECLIP, sctrl).getValue())
                index = np.where((res > (resMean + nSigmaClip * resStd)) | (res < resMean - nSigmaClip * resStd))
                self.log.info("%.3f %.3f %.3f %.3f"%(resMean, resStd, np.max(res), nSigmaClip))
                if np.shape(np.where(index))[1] == 0 or (nIter >= maxIter):  # run out of points, or iterations
                    break
                x = np.delete(x, index)
                y = np.delete(y, index)

            return slope, 0

        while nIter < maxIter:  # TODO: change log levels to debug
            nIter += 1
            self.log.info("Iteration # %s, %s elements:"%(nIter, np.shape(x)[0]))
            xx = np.vstack([x, np.ones(len(x))]).T
            ret, _, _, _ = np.linalg.lstsq(xx, y)
            slope, intercept = ret
            res = y - slope*x - intercept
            resMean = afwMath.makeStatistics(res, afwMath.MEANCLIP, sctrl).getValue()
            resStd = np.sqrt(afwMath.makeStatistics(res, afwMath.VARIANCECLIP, sctrl).getValue())
            index = np.where((res > (resMean + nSigmaClip * resStd)) | (res < resMean - nSigmaClip * resStd))
            self.log.info("%.3f %.3f %.3f %.3f"%(resMean, resStd, np.max(res), nSigmaClip))
            if np.shape(np.where(index))[1] == 0 or (nIter >= maxIter):  # run out of points, or iterations
                break
            x = np.delete(x, index)
            y = np.delete(y, index)

        return slope, intercept

    def _generateKernel(self, corrs, means, rejectLevel=None):
        """Generate the full kernel from a list of (gain-corrected) cross-correlations and means.

        Taking a list of quarter-image, gain-corrected cross-correlations, do a pixel-wise sigma-clipped
        mean of each, and tile into the full-sized kernel image.

        Each corr in corrs is one quarter of the full cross-correlation, and has been gain-corrected.
        Each mean in means is a tuple of the means of the two individual images, corresponding to that corr.

        Parameters:
        -----------
        corrs : `list` of `numpy.ndarray`, (Ny, Nx)
            A list of the quarter-image cross-correlations
        means : `list` of `tuples` of `floats`
            The means of the input images for each corr in corrs
        rejectLevel : `float`, optional
            This is essentially is a sanity check parameter.
            If this condition is violated there is something unexpected going on in the image, and it is
            discarded from the stack before the clipped-mean is calculated.

        Returns:
        --------
        kernel : `numpy.ndarray`, (Ny, Nx)
            The output kernel
        """
        if not rejectLevel:
            rejectLevel = self.config.xcorrCheckRejectLevel

        if not isinstance(corrs, list):  # we expect a list of arrays
            corrs = [corrs]

        # Try to average over a set of possible inputs. This generates a simple function of the kernel that
        # should be constant across the images, and averages that.
        xcorrList = []
        sctrl = afwMath.StatisticsControl()
        sctrl.setNumSigmaClip(self.config.nSigmaClipKernelGen)

        for corrNum, ((mean1, mean2), corr) in enumerate(zip(means, corrs)):
            corr[0, 0] -= (mean1+mean2)
            if corr[0, 0] > 0:
                self.log.warn('Skipped item %s due to unexpected value of (variance-mean)!'%corrNum)
                continue
            corr /= -float(1.0*(mean1**2+mean2**2))

            fullCorr = self._tileArray(corr)

            # TODO: what is this block really testing? Is this what it should be doing? First line is fishy
            xcorrCheck = np.abs(np.sum(fullCorr))/np.sum(np.abs(fullCorr))
            if xcorrCheck > rejectLevel:
                self.log.warn("Sum of the xcorr is unexpectedly high. Investigate item num %s. \n"
                              "value = %s"%(corrNum, xcorrCheck))
                continue
            xcorrList.append(fullCorr)

        if not xcorrList:
            raise RuntimeError("Cannot generate kernel because all inputs were discarded. "
                               "Either the data is bad, or config.xcorrCheckRejectLevel is too low")

        # stack the individual xcorrs and apply a per-pixel clipped-mean
        meanXcorr = np.zeros_like(fullCorr)
        xcorrList = np.transpose(xcorrList)
        for i in range(np.shape(meanXcorr)[0]):
            for j in range(np.shape(meanXcorr)[1]):
                meanXcorr[i, j] = afwMath.makeStatistics(xcorrList[i, j], afwMath.MEANCLIP, sctrl).getValue()

        return self._SOR(meanXcorr)

    def _SOR(self, source, maxIter=None, eLevel=None):
        """An implementation of the successive over relaxation (SOR) method.

        Parameters:
        -----------
        source : `numpy.ndarray`, (Ny, Nx)
            The input array
        maxIter : `int`, optional
            Maximum number of iterations to attempt before aborting
        eLevel : `float`, optional
            The target error level factor at which we deem convergence to have occured

        Returns:
        --------
        output : `numpy.ndarray`, (Ny, Nx)
            The solution
        """
        if not maxIter:
            maxIter = self.config.maxIterSOR
        if not eLevel:
            eLevel = self.config.eLevelSOR

        # initialise, and set boundary conditions
        func = np.zeros([source.shape[0]+2, source.shape[1]+2])
        resid = np.zeros([source.shape[0]+2, source.shape[1]+2])
        rhoSpe = np.cos(np.pi/source.shape[0])  # Here a square grid is assummed

        inError = 0
        # Calculate the initial error
        for i in range(1, func.shape[0]-1):
            for j in range(1, func.shape[1]-1):
                resid[i, j] = (func[i, j-1]+func[i, j+1]+func[i-1, j] +
                               func[i+1, j]-4*func[i, j]-source[i-1, j-1])
        inError = np.sum(np.abs(resid))

        # Iterate until convergence
        # We perform two sweeps per cycle, updating 'odd' and 'even' points separately
        nIter = 0
        omega = 1.0
        dx = 1.0
        while nIter < maxIter*2:
            outError = 0
            if nIter%2 == 0:
                for i in range(1, func.shape[0]-1, 2):
                    for j in range(1, func.shape[0]-1, 2):
                        resid[i, j] = float(func[i, j-1]+func[i, j+1]+func[i-1, j] +
                                            func[i+1, j]-4.0*func[i, j]-dx*dx*source[i-1, j-1])
                        func[i, j] += omega*resid[i, j]*.25
                for i in range(2, func.shape[0]-1, 2):
                    for j in range(2, func.shape[0]-1, 2):
                        resid[i, j] = float(func[i, j-1]+func[i, j+1]+func[i-1, j] +
                                            func[i+1, j]-4.0*func[i, j]-dx*dx*source[i-1, j-1])
                        func[i, j] += omega*resid[i, j]*.25
            else:
                for i in range(1, func.shape[0]-1, 2):
                    for j in range(2, func.shape[0]-1, 2):
                        resid[i, j] = float(func[i, j-1]+func[i, j+1]+func[i-1, j] +
                                            func[i+1, j]-4.0*func[i, j]-dx*dx*source[i-1, j-1])
                        func[i, j] += omega*resid[i, j]*.25
                for i in range(2, func.shape[0]-1, 2):
                    for j in range(1, func.shape[0]-1, 2):
                        resid[i, j] = float(func[i, j-1]+func[i, j+1]+func[i-1, j] +
                                            func[i+1, j]-4.0*func[i, j]-dx*dx*source[i-1, j-1])
                        func[i, j] += omega*resid[i, j]*.25
            outError = np.sum(np.abs(resid))
            if outError < inError*eLevel:
                break
            if nIter == 0:
                omega = 1.0/(1-rhoSpe*rhoSpe/2.0)
            else:
                omega = 1.0/(1-rhoSpe*rhoSpe*omega/4.0)
            nIter += 1

        if nIter >= maxIter*2:
            self.log.warn("Did not converge in %s iterations.\noutError: %s, inError: "
                          "%s,"%(nIter//2, outError, inError*eLevel))
        else:
            self.log.info("Converged in %s iterations.\noutError: %s, inError: "
                          "%s", nIter//2, outError, inError*eLevel)
        return func[1:-1, 1:-1]

    # This sim code is used to estimate the bias correction used above.
    def xcorr_sim(self, im, im2, n=8, border=10, sigma=5):
        """Perform a simple xcorr from two images.

        It contains many elements of the actual code
        above (without individual amps and ISR removal )
        It takes two images, im and im2; n the max lag of the correlation function; border, the number of border
        pixels to discard; and sigma the sigma to use in the mean clip.
        """
        sctrl = afwMath.StatisticsControl()
        sctrl.setNumSigmaClip(sigma)

        for attr in ("getMaskedImage", "getImage"):
            if hasattr(im, attr):
                im = getattr(im, attr)()
            if hasattr(im2, attr):
                im2 = getattr(im2, attr)()

        try:
            im = im.convertF()
            im2 = im2.convertF()
        except AttributeError:
            pass
        means1 = [0, 0]
        means1[0] = afwMath.makeStatistics(im[border:-border, border:-border],
                                           afwMath.MEANCLIP, sctrl).getValue()
        means1[1] = afwMath.makeStatistics(im2[border:-border, border:-border],
                                           afwMath.MEANCLIP, sctrl).getValue()
        im -= means1[0]
        im2 -= means1[1]
        diff = im2.clone()
        diff -= im.clone()
        diff = diff[border:-border, border:-border]
        binsize = 128
        nx = diff.getWidth()//binsize
        ny = diff.getHeight()//binsize
        bctrl = afwMath.BackgroundControl(nx, ny, sctrl, afwMath.MEANCLIP)
        bkgd = afwMath.makeBackground(diff, bctrl)
        diff -= bkgd.getImageF(afwMath.Interpolate.CUBIC_SPLINE, afwMath.REDUCE_INTERP_ORDER)
        dim0 = diff[0: -n, : -n].clone()
        dim0 -= afwMath.makeStatistics(dim0, afwMath.MEANCLIP, sctrl).getValue()
        w, h = dim0.getDimensions()
        xcorr = afwImage.ImageD(n + 1, n + 1)
        for di in range(n + 1):
            for dj in range(n + 1):
                dim_ij = diff[di:di + w, dj: dj + h].clone()
                dim_ij -= afwMath.makeStatistics(dim_ij, afwMath.MEANCLIP, sctrl).getValue()

                dim_ij *= dim0
                xcorr[di, dj] = afwMath.makeStatistics(dim_ij, afwMath.MEANCLIP, sctrl).getValue()
        L = np.shape(xcorr.getArray())[0]-1
        XCORR = np.zeros([2*L+1, 2*L+1])
        for i in range(L+1):
            for j in range(L+1):
                XCORR[i+L, j+L] = xcorr.getArray()[i, j]
                XCORR[-i+L, j+L] = xcorr.getArray()[i, j]
                XCORR[i+L, -j+L] = xcorr.getArray()[i, j]
                XCORR[-i+L, -j+L] = xcorr.getArray()[i, j]
        # print((means1),xcorr.getArray()[0,0],np.sum(XCORR),xcorr.getArray()[0,0]/
        #       (np.sum(means1)),np.sum(XCORR)/(np.sum(means1)))
        return (XCORR, xcorr, np.sum(means1), means1)

    def xcorr_bias(self, rangeMeans=[87500, 70000, 111000], repeats=5, sig=5,
                   border=3, seed=None, nx=2000, ny=4000, case=0, a=.1):
        """Fill images of specified size (nx and ny) with poisson points with means (in rangeMeans).

        before passing it to the above function with border and sig as above
        Repeats specifies the number of times to run the simulations.
        If case is 1 then a correlation between x_{i,j} and x_{i+1,j+1} is artificially introduced
        by adding a*x_{i,j} to x_{i+1,j+1}
        If seed is left to None the seed with be pulled from /dev/random.
        Else an int can be passed to see the random number generator.
        """
        if seed is None:
            with open("/dev/random", 'rb') as file:
                local_random = np.random.RandomState(int(file.read(4).encode('hex'), 16))
        else:
            local_random = np.random.RandomState(int(seed))
        MEANS = {}
        XCORRS = {}
        for M in rangeMeans:
            MEANS[M] = []
            XCORRS[M] = []

        if not case:
            for rep in range(repeats):
                for i, MEAN in enumerate(rangeMeans):

                    im = afwImage.ImageD(nx, ny)
                    im0 = afwImage.ImageD(nx, ny)
                    # im.getArray()[:,:]=local_random.normal(MEAN,np.sqrt(MEAN),(ny,nx))
                    # im0.getArray()[:,:]=local_random.normal(MEAN,np.sqrt(MEAN),(ny,nx))
                    im.getArray()[:, :] = local_random.poisson(MEAN, (ny, nx))
                    im0.getArray()[:, :] = local_random.poisson(MEAN, (ny, nx))
                    XCORR, xcorr, means, MEANS1 = xcorr_sim(im, im0, border=border, sigma=sig)
                    MEANS[MEAN].append(means)
                    XCORRS[MEAN].append(xcorr)
                print('\n\n\n')
                for i, MEAN in enumerate(rangeMeans):
                    print("Simulated/Expected:", MEAN, MEANS[MEAN][-1],
                          XCORRS[MEAN][-1].getArray()[0, 0]/MEANS[MEAN][-1])
        else:
            for rep in range(repeats):
                for i, MEAN in enumerate(rangeMeans):
                    im = afwImage.ImageD(nx, ny)
                    im0 = afwImage.ImageD(nx, ny)
                    # im.getArray()[:,:]=local_random.normal(MEAN,np.sqrt(MEAN),(ny,nx))
                    # im0.getArray()[:,:]=local_random.normal(MEAN,np.sqrt(MEAN),(ny,nx))
                    im.getArray()[:, :] = local_random.poisson(MEAN, (ny, nx))
                    im.getArray()[1:, 1:] += a*im.getArray()[:-1, :-1]
                    im0.getArray()[:, :] = local_random.poisson(MEAN, (ny, nx))
                    im0.getArray()[1:, 1:] += a*im0.getArray()[:-1, :-1]
                    XCORR, xcorr, means, MEANS1 = xcorr_sim(im, im0, border=border, sigma=sig)
                    MEANS[MEAN].append(means)
                    XCORRS[MEAN].append(xcorr)
                print('\n\n\n')
                for i, MEAN in enumerate(rangeMeans):
                    print("Simulated/Expected:", MEANS[MEAN][-1], '\n',
                          (XCORRS[MEAN][-1].getArray()[1, 1]/MEANS[MEAN][-1]*(1+a))/.1)
        return MEANS, XCORRS

    @staticmethod
    def _tileArray(in_array):
        """Given a square input quarter-image, tile/mirror it, returning the full image.

        Given an input of side-length n, of the form

        input = array([[1, 2, 3],
                       [4, 5, 6],
                       [7, 8, 9]])

        return an array of size 2n-1 as

        output = array([[ 9,  8,  7,  8,  9],
                        [ 6,  5,  4,  5,  6],
                        [ 3,  2,  1,  2,  3],
                        [ 6,  5,  4,  5,  6],
                        [ 9,  8,  7,  8,  9]])

        Parameters:
        -----------
        input : `np.array`
            The square input quarter-array

        Returns:
        --------
        output : `np.array`
            The full, tiled array
        """
        assert(in_array.shape[0] == in_array.shape[1])
        length = in_array.shape[0]-1
        output = np.zeros((2*length+1, 2*length+1))

        for i in range(length+1):
            for j in range(length+1):
                output[i+length, j+length] = in_array[i, j]
                output[-i+length, j+length] = in_array[i, j]
                output[i+length, -j+length] = in_array[i, j]
                output[-i+length, -j+length] = in_array[i, j]
        return output

    def _xcorr(self, im1, im2, gains):
        """Calculate the cross-correlation of two images im1 and im2 using robust measures of the covariance.

        TODO: Write a proper docstring here

        Maximum lag is maxLag, and ignore border pixels around the outside.
        Sigma is the number of sigma passed to sig cut.
        GAIN allows user specified GAINS to be used otherwise the default gains are used.
        The biasCorr parameter is used to correct from the bias of our measurements introduced by the sigma cuts.
        This was calculated using the sim. code at the bottom.
        This function returns one quater of the correlation function, the sum of the means of the two images and
        the individual means of the images
        """
        maxLag = self.config.maxLag
        border = self.config.nPixBorderXCorr
        sigma = self.config.nSigmaClipXCorr
        biasCorr = self.config.biasCorr

        sctrl = afwMath.StatisticsControl()
        sctrl.setNumSigmaClip(sigma)

        means = [None, None]
        means1 = [None, None]
        for imNum, im in enumerate([im1, im2]):
            ccd = im.getDetector()
            try:
                frameId = int(re.sub(r"^SUPA0*", "", im.getMetadata().get("FRAMEID")))
            except:
                frameId = -1
            #
            # Starting with an Exposure, MaskedImage, or Image trim the data and convert to float
            #
            for attr in ("getMaskedImage", "getImage"):
                if hasattr(im, attr):
                    im = getattr(im, attr)()
            try:
                im = im.convertF()
            except AttributeError:
                pass
            # im = trim(im, ccd)
            means[imNum] = afwMath.makeStatistics(im[border:-border, border:-border],
                                                  afwMath.MEANCLIP, sctrl).getValue()
            temp = im.clone()
            # Rescale each amp by the appropriate gain and subtract the mean.
            for ampNum, amp in enumerate(ccd):
                # smi = im[amp.getDataSec(True)]
                # smiTemp = temp[amp.getDataSec(True)]
                smi = im[amp.getBBox()]
                smiTemp = temp[amp.getBBox()]
                mean = afwMath.makeStatistics(smi, afwMath.MEANCLIP, sctrl).getValue()
                gain = gains[ampNum]
                # gain/=gain
                smi *= gain
                print(mean*gain, afwMath.makeStatistics(smi, afwMath.MEANCLIP, sctrl).getValue())
                smi -= mean*gain
                smiTemp *= gain
            means1[imNum] = afwMath.makeStatistics(temp[border:-border, border:-border],
                                                   afwMath.MEANCLIP, sctrl).getValue()
            print(afwMath.makeStatistics(temp[border:-border, border:-border],
                                         afwMath.MEANCLIP, sctrl).getValue())
        #    print(afwMath.makeStatistics(temp, afwMath.MEANCLIP,sctrl).getValue()-
        #          afwMath.makeStatistics(temp[0:-maxLag,0:-maxLag], afwMath.MEANCLIP,sctrl).getValue())

        #
        # Actually diff the images
        #
        diff = im1.clone()
        diff = diff.getMaskedImage().getImage()
        diff -= im2.getMaskedImage().getImage()

        diff = diff[border:-border, border:-border]
        # diff.writeFits("./Data/Diff_CCD_"+str(CCD)+".fits")
        #
        # Subtract background.  It should be a constant, but it isn't always
        #
        binsize = 128
        nx = diff.getWidth()//binsize
        ny = diff.getHeight()//binsize
        bctrl = afwMath.BackgroundControl(nx, ny, sctrl, afwMath.MEANCLIP)
        bkgd = afwMath.makeBackground(diff, bctrl)
        diff -= bkgd.getImageF(afwMath.Interpolate.CUBIC_SPLINE, afwMath.REDUCE_INTERP_ORDER)

        if False:
            ds9.mtv(diff, frame=frame, title="diff")

        if False:
            global diffim
            diffim = diff
        if True:
            print("median and variance of diff:")
            print(afwMath.makeStatistics(diff, afwMath.MEDIAN, sctrl).getValue())
            print(afwMath.makeStatistics(diff, afwMath.VARIANCECLIP, sctrl).getValue(), np.var(diff.getArray()))
        #
        # Measure the correlations
        #
        dim0 = diff[0: -maxLag, : -maxLag]
        dim0 -= afwMath.makeStatistics(dim0, afwMath.MEANCLIP, sctrl).getValue()
        w, h = dim0.getDimensions()
        xcorr = np.zeros((maxLag + 1, maxLag + 1), dtype=np.float64)

        for xlag in range(maxLag + 1):
            for ylag in range(maxLag + 1):
                dim_xy = diff[xlag:xlag + w, ylag: ylag + h].clone()
                dim_xy -= afwMath.makeStatistics(dim_xy, afwMath.MEANCLIP, sctrl).getValue()
                dim_xy *= dim0
                xcorr[xlag, ylag] = afwMath.makeStatistics(dim_xy, afwMath.MEANCLIP, sctrl).getValue()/(biasCorr)

        xcorr_full = self._tileArray(xcorr)
        print(sum(means1), xcorr[0, 0], np.sum(xcorr_full), xcorr[0, 0]/sum(means1),
              np.sum(xcorr_full)/sum(means1))
        return (xcorr, means1)
